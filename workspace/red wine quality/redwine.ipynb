{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5188c5b-ae79-40c8-a2ab-c76d910900a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dataset base path: /home/jovyan/data/red-wine-quality-cortez-et-al-2009\n",
      "Raw dataset already exists at /home/jovyan/data/red-wine-quality-cortez-et-al-2009/raw\n",
      "Skipping download...\n",
      "Raw dataset saved to persistent volume at: /home/jovyan/data/red-wine-quality-cortez-et-al-2009/raw\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_id = \"uciml/red-wine-quality-cortez-et-al-2009\"\n",
    "local_dataset_name = dataset_id.split('/')[-1] # Uses the last part of the ID\n",
    "\n",
    "\n",
    "base_data_dir = Path(\"/home/jovyan/data\") # Standard for jovyan user\n",
    "# base_data_dir = project_root / \"data\" # Alternative for local non-Docker runs\n",
    "dataset_specific_base_path = base_data_dir / local_dataset_name\n",
    "raw_data_target_dir = dataset_specific_base_path / \"raw\"\n",
    "print(f\"Target dataset base path: {dataset_specific_base_path}\")\n",
    "\n",
    "# expected_raw_file_name = \"winequality-red.csv\"\n",
    "# final_raw_file_path = raw_data_target_dir / expected_raw_file_name\n",
    "# print(f\"Target raw data file: {final_raw_file_path}\")\n",
    "\n",
    "# Check if dataset already exists in destination\n",
    "destination_path = f\"/home/jovyan/data/{local_dataset_name}\"\n",
    "if os.path.exists(raw_data_target_dir) and os.listdir(dataset_specific_base_path):\n",
    "    print(f\"Raw dataset already exists at {raw_data_target_dir}\")\n",
    "    print(\"Skipping download...\")\n",
    "else:\n",
    "    print(f\"Dataset not found locally. Downloading {dataset_id}...\")\n",
    "    download_path = kagglehub.dataset_download(dataset_id)\n",
    "    print(f\"Dataset downloaded to temporary path in container: {download_path}\")\n",
    "    print(f\"Copying dataset to persistent raw data volume: {raw_data_target_dir}\")\n",
    "    # Ensure the destination directory exists \n",
    "    os.makedirs(raw_data_target_dir, exist_ok=True)\n",
    "    for item in os.listdir(download_path):\n",
    "        s = os.path.join(download_path, item)\n",
    "        d = os.path.join(raw_data_target_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, symlinks=False, ignore=None, dirs_exist_ok=True)\n",
    "        else:\n",
    "            shutil.copy2(s, d)\n",
    "\n",
    "    print(f\"Dataset '{dataset_id}' successfully copied to {raw_data_target_dir} in shared volume.\")\n",
    "print(f\"Raw dataset saved to persistent volume at: {raw_data_target_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef36c95-81dc-45b5-a91a-6134692ce15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files in /home/jovyan/data/red-wine-quality-cortez-et-al-2009/raw:\n",
      "raw/\n",
      "    winequality-red.csv\n",
      "\n",
      "Loading data from: /home/jovyan/data/red-wine-quality-cortez-et-al-2009/raw/winequality-red.csv\n",
      "\n",
      "Available columns in the RAW dataset:\n",
      "0: fixed acidity\n",
      "1: volatile acidity\n",
      "2: citric acid\n",
      "3: residual sugar\n",
      "4: chlorides\n",
      "5: free sulfur dioxide\n",
      "6: total sulfur dioxide\n",
      "7: density\n",
      "8: pH\n",
      "9: sulphates\n",
      "10: alcohol\n",
      "11: quality\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Import Libraries and Load Data ---\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os\n",
    "import cProfile # For basic profiling\n",
    "import pstats # For processing profiling results\n",
    "import io # For capturing profiling output\n",
    "\n",
    "print(f\"Listing files in {raw_data_target_dir}:\")\n",
    "try:\n",
    "    for root, dirs, files in os.walk(raw_data_target_dir):\n",
    "        level = root.replace(str(raw_data_target_dir), '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f'{indent}{os.path.basename(root)}/')\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f'{subindent}{f}')\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Directory not found: {raw_data_target_dir}. Please ensure the dataset was downloaded and copied correctly.\")\n",
    "    # Exit or handle the error appropriately if the directory is not found\n",
    "\n",
    "csv_files = [f for f in os.listdir(raw_data_target_dir) if f.endswith('.csv')]\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {raw_data_target_dir}\")\n",
    "if len(csv_files) > 1:\n",
    "    print(f\"Warning: Multiple CSV files found. Using the first one: {csv_files[0]}\")\n",
    "\n",
    "# Load the data\n",
    "data_file_path = os.path.join(raw_data_target_dir, csv_files[0])\n",
    "print(f\"\\nLoading data from: {data_file_path}\")\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "print(\"\\nAvailable columns in the RAW dataset:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i}: {col}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb37035f-18d8-483b-acb9-9251a4944b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/jovyan/data/red-wine-quality-cortez-et-al-2009/raw/winequality-red.csv')]\n",
      "Dataset loaded successfully. Only one file loaded\n",
      "Dataset shape: (1599, 12)\n",
      "Dataset columns: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "Dataset head:\n",
      "    fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n",
      "Features shape: (1599, 11)\n",
      "Target shape: (1599,)\n",
      "ETL Version: v1\n",
      "ETL Description: Default ETL logic\n",
      "X_train shape: (1279, 11), y_train shape: (1279,)\n",
      "X_val shape: (320, 11), y_val shape: (320,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "\n",
    "from utils.etl import run_etl\n",
    "# import inspect\n",
    "target_column = \"quality\"\n",
    "etl_result = run_etl(dataset_name=local_dataset_name, target_column=target_column)\n",
    "\n",
    "X_train = etl_result[\"X_train\"]\n",
    "y_train = etl_result[\"y_train\"]\n",
    "X_val = etl_result[\"X_val\"]\n",
    "y_val = etl_result[\"y_val\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3716c353-055c-4f92-8086-64d336940c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL Version: v1\n",
      "ETL Description: Default ETL logic\n",
      "X_train shape: (1279, 11), y_train shape: (1279,)\n",
      "X_val shape: (320, 11), y_val shape: (320,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"ETL Version: {etl_result['etl_version']}\")\n",
    "print(f\"ETL Description: {etl_result['etl_description']}\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625f5ac-854e-49e8-a259-265310fdbec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import get_sklearn_model, get_nn_model\n",
    "from utils.train import get_next_train_script, train_sklearn_model, train_nn_model\n",
    "\n",
    "import mlflow\n",
    "os.environ[\"WANDB_BASE_URL\"] = \"http://wandb:8080\"\n",
    "os.environ[\"WANDB_DEBUG\"] = \"true\"\n",
    "os.environ[\"WANDB_DEBUG_LOG_PATH\"] = \"/tmp/wandb_debug.log\"\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def log_model_artifact(model, model_name, framework=\"sklearn\"):\n",
    "    artifact_path = f\"{model_name}\"\n",
    "    \n",
    "    if framework == \"sklearn\":\n",
    "        import joblib\n",
    "        fname = f\"{artifact_path}.pkl\"\n",
    "        joblib.dump(model, fname)\n",
    "        mlflow.sklearn.log_model(model, artifact_path=\"model\")\n",
    "    elif framework == \"torch\":\n",
    "        import torch\n",
    "        fname = f\"{artifact_path}.pt\"\n",
    "        torch.save(model.state_dict(), fname)\n",
    "        mlflow.pytorch.log_model(model, artifact_path=\"model\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported framework\")\n",
    "\n",
    "    artifact = wandb.Artifact(artifact_path, type=\"model\")\n",
    "    artifact.add_file(fname)\n",
    "    wandb.log_artifact(artifact)\n",
    "    \n",
    "def log_metrics_and_params(trained_model, model_name, params, task_type, eval_metrics):\n",
    "    # MLflow\n",
    "    mlflow.set_experiment(f\"kaggle_{local_dataset_name}\")\n",
    "    mlflow.start_run()\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_param(\"task_type\", task_type)\n",
    "    for metric, value in eval_metrics.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            mlflow.log_metric(metric, value)\n",
    "    \n",
    "    # W&B\n",
    "    wandb.init(\n",
    "        project=f\"kaggle_{local_dataset_name}\",\n",
    "        name=f\"{model_name}\",\n",
    "        config={**params, \"model_name\": model_name, \"task_type\": task_type}\n",
    "    )\n",
    "    wandb.log(eval_metrics)\n",
    "\n",
    "    # write model artifacts, commented out to reduce write2disk\n",
    "    # framework = \"torch\" if model_name == \"neural_net\" else \"sklearn\"\n",
    "    # log_model_artifact(trained_model, model_name, train_version, framework)\n",
    "    \n",
    "    wandb.finish()\n",
    "    mlflow.end_run()\n",
    "\n",
    "def log_final_metrics(eval_metrics):\n",
    "    for metric, value in eval_metrics.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            mlflow.log_metric(metric, value)\n",
    "    wandb.log(eval_metrics)\n",
    "    \n",
    "# Define model sweep\n",
    "task_type = \"classification\"\n",
    "model_configs = {\n",
    "    \"decision_tree\": [\n",
    "        {\"max_depth\": 3},\n",
    "        # {\"max_depth\": 5}\n",
    "    ],\n",
    "    # \"random_forest\": [\n",
    "    #     {\"n_estimators\": 50},\n",
    "    #     {\"n_estimators\": 100}\n",
    "    # ],\n",
    "    \"neural_net\": [\n",
    "        {\"hidden\": 32},\n",
    "        # {\"hidden\": 64}\n",
    "    ]\n",
    "}\n",
    "#XGBoost, LightGBM ?\n",
    "\n",
    "# Loop over each model and its hyperparam\n",
    "for model_name, config_list in model_configs.items():\n",
    "    for params in config_list:\n",
    "        # === Init Experiment | MLflow and W&B ===\n",
    "        mlflow.set_experiment(f\"kaggle_{local_dataset_name}\")\n",
    "        mlflow.start_run()\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"task_type\", task_type)\n",
    "        wandb_run.init(\n",
    "            project=f\"kaggle_{local_dataset_name}\",\n",
    "            name=f\"{model_name}\",\n",
    "            config={**params, \"model_name\": model_name, \"task_type\": task_type}\n",
    "        )\n",
    "        \n",
    "        # === Train ===\n",
    "        if model_name == \"neural_net\":\n",
    "            model = get_nn_model(X_train.shape[1], **params)\n",
    "            trained_model, y_pred = = train_nn_model(model, X_train, y_train, X_val, y_val) #needs updating for designating eval_metric and eval_metric_func\n",
    "        else:\n",
    "            model = get_sklearn_model(model_name, **params)\n",
    "            trained_model, y_pred = = train_sklearn_model(model, X_train, y_train, X_val, y_val)\n",
    "            \n",
    "        # === Evaluate & log ===\n",
    "        eval_metrics = evaluate_model(y_val, y_pred, task_type)\n",
    "        log_final_metrics(eval_metrics)\n",
    "        log_model_artifact(trained_model, model_name, framework=\"torch\" if model_name == \"neural_net\" else \"sklearn\")\n",
    "\n",
    "        # === End Experiment | MLflow and W&B ===\n",
    "        mlflow.end_run()\n",
    "        wandb.finish()\n",
    "\n",
    "        # log_metrics_and_params(\n",
    "        #     trained_model = trained_model, #not used for now to reduce write to disk\n",
    "        #     model_name=model_name,\n",
    "        #     params=params,\n",
    "        #     task_type=task_type,\n",
    "        #     eval_metrics=eval_metrics\n",
    "        # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc7a0745-05b9-44a8-a82d-6be6d2f23462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting MLflow experiment: red-wine-quality-cortez-et-al-2009_Decision_Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: http://wandb:8080/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow run already active with ID: 040dca790add4ad4b16591e41ad8e0fb\n",
      "Get wandb API key from localhost:8082/authorize\n",
      "Initializing W&B run: Project='kaggle_red-wine-quality-cortez-et-al-2009', Name='decision-tree-training'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for host.docker.internal:8082 to your netrc file: /home/jovyan/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged in\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/wandb/run-20250514_181906-yeb8ztxn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009/runs/yeb8ztxn' target=\"_blank\">decision-tree-training</a></strong> to <a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009' target=\"_blank\">http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009/runs/yeb8ztxn' target=\"_blank\">http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009/runs/yeb8ztxn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started W&B run with ID: yeb8ztxn\n",
      "Logging parameters to MLflow and W&B...\n",
      "Parameters logged.\n",
      "Starting profiling...\n",
      "Training Decision Tree model...\n",
      "Model training complete.\n",
      "Stopping profiling...\n",
      "Profiling stopped.\n",
      "Processing profiling results...\n",
      "Profiling results processed.\n",
      "\n",
      "--- Profiling Snippet (Top 10 by Cumulative Time) ---\n",
      "         3939 function calls (3889 primitive calls) in 0.009 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        6    0.000    0.000    0.009    0.002 /opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3490(run_code)\n",
      "        6    0.000    0.000    0.009    0.002 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.009    0.009 /opt/conda/lib/python3.11/site-packages/sklearn/base.py:1372(wrapper)\n",
      "        1    0.000    0.000    0.008    0.008 /opt/conda/lib/python3.11/site-packages/sklearn/tree/_classes.py:993(fit)\n",
      "        1    0.006    0.006    0.008    0.008 /opt/conda/lib/python3.11/site-packages/sklearn/tree/_classes.py:231(_fit)\n",
      "        1    0.000    0.000    0.002    0.002 /opt/conda/lib/python3.11/site-packages/sklearn/utils/validation.py:2835(validate_data)\n",
      "        4    0.000    0.000    0.002    0.000 /opt/conda/lib/python3.11/site-packages/sklearn/utils/validation.py:736(check_array)\n",
      "        8    0.000    0.000    0.001    0.000 /opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:6432(dtypes)\n",
      "        9    0.000    0.000    0.001    0.000 /opt/conda/lib/python3.11/site-packages/pandas/core/series.py:389(__init__)\n",
      "      145    0.000    0.000    0.001    0.000 {built-in method builtins.hasattr}\n",
      "----------------------------------------------------\n",
      "Making predictions on the test set...\n",
      "Predictions made.\n",
      "Calculating evaluation metrics...\n",
      "Accuracy: 0.5531\n",
      "Precision: 0.5320\n",
      "Recall: 0.5531\n",
      "F1 Score: 0.5409\n",
      "Metrics calculated.\n",
      "Logging metrics to MLflow and W&B...\n",
      "Metrics logged.\n",
      "Logging model with MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/14 18:19:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged to MLflow.\n",
      "Logging profiling results as artifacts...\n",
      "Profiling results logged as artifact: profiling_results.txt\n",
      "Temporary profiling file removed: profiling_results.txt\n",
      "Ending MLflow run...\n",
      "üèÉ View run omniscient-conch-714 at: http://mlflow:5000/#/experiments/212578242154088023/runs/040dca790add4ad4b16591e41ad8e0fb\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/212578242154088023\n",
      "MLflow run ended.\n",
      "Ending W&B run...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>f1_score</td><td>‚ñÅ</td></tr><tr><td>precision</td><td>‚ñÅ</td></tr><tr><td>recall</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.55312</td></tr><tr><td>f1_score</td><td>0.5409</td></tr><tr><td>precision</td><td>0.53197</td></tr><tr><td>recall</td><td>0.55312</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decision-tree-training</strong> at: <a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009/runs/yeb8ztxn' target=\"_blank\">http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009/runs/yeb8ztxn</a><br> View project at: <a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009' target=\"_blank\">http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/home/jovyan/wandb/wandb/run-20250514_181906-yeb8ztxn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run finished.\n",
      "\n",
      "Experiment complete. Check MLflow UI at http://localhost:5000 and W&B UI at http://localhost:8082\n"
     ]
    }
   ],
   "source": [
    "# --- MLflow and W&B Setup, Profiling Start ---\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "os.environ[\"WANDB_BASE_URL\"] = \"http://wandb:8080\"\n",
    "os.environ[\"WANDB_DEBUG\"] = \"true\"\n",
    "os.environ[\"WANDB_DEBUG_LOG_PATH\"] = \"/tmp/wandb_debug.log\"\n",
    "import wandb\n",
    "# Ensure MLflow tracking URI is set (should be from environment variable)\n",
    "\n",
    "\n",
    "etl_version = etl_result[\"etl_version\"]\n",
    "etl_description = etl_result[\"etl_description\"]\n",
    "# etl_script_path = etl_result.get(\"etl_script_path\", \"N/A\")\n",
    "\n",
    "train_version = \"1\"\n",
    "train_description = \"\"\n",
    "mlflow_experiment_name = f\"{local_dataset_name}_ETLAlgo_{etl_version}_TrainAlgo_{train_version}\"\n",
    "\n",
    "print(f\"\\nSetting MLflow experiment: {mlflow_experiment_name}\")\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "\n",
    "mlflow.log_param(\"dataset\", local_dataset_name)\n",
    "mlflow.log_param(\"etl_version\", etl_version)\n",
    "mlflow.log_param(\"etl_description\", etl_description)\n",
    "\n",
    "# Only start a new run if none is active\n",
    "active_run = mlflow.active_run()\n",
    "if active_run is None:\n",
    "    mlflow_run = mlflow.start_run()\n",
    "    print(f\"Started new MLflow run with ID: {mlflow_run.info.run_id}\")\n",
    "else:\n",
    "    print(f\"MLflow run already active with ID: {active_run.info.run_id}\")\n",
    "    \n",
    "\n",
    "# W&B: Initialize a new run\n",
    "# The project name helps organize runs in the W&B UI\n",
    "# The WANDB_DIR environment variable in docker-compose.yml ensures data goes to the shared volume\n",
    "wandb_project_name = f\"kaggle_{local_dataset_name}\"\n",
    "wandb_run_name = \"decision-tree-training\"\n",
    "print(\"Get wandb API key from localhost:8082/authorize\")\n",
    "print(f\"Initializing W&B run: Project='{wandb_project_name}', Name='{wandb_run_name}'\")\n",
    "wandb.login(relogin=True, host=\"http://host.docker.internal:8082\")\n",
    "wandb.init(project=wandb_project_name, name=wandb_run_name)\n",
    "wandb.config.update({\n",
    "    \"dataset\": local_dataset_name,\n",
    "    \"etl_version\": etl_version,\n",
    "    \"etl_description\": etl_description,\n",
    "})\n",
    "\n",
    "print(f\"Started W&B run with ID: {wandb.run.id}\")\n",
    "\n",
    "# Define model parameters\n",
    "max_depth = 4 # Example hyperparameter\n",
    "random_state = 20\n",
    "\n",
    "# Log parameters to MLflow and W&B\n",
    "print(\"Logging parameters to MLflow and W&B...\")\n",
    "mlflow.log_param(\"max_depth\", max_depth)\n",
    "mlflow.log_param(\"random_state\", random_state)\n",
    "wandb.config.max_depth = max_depth\n",
    "wandb.config.random_state = random_state\n",
    "print(\"Parameters logged.\")\n",
    "\n",
    "# --- Start Profiling ---\n",
    "# Profiling the training process to understand where time is spent\n",
    "print(\"Starting profiling...\")\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "\n",
    "\n",
    "# --- Cell 4: Model Training ---\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "print(\"Training Decision Tree model...\")\n",
    "model = DecisionTreeClassifier(max_depth=max_depth, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "\n",
    "# --- Cell 5: Profiling Stop and Processing ---\n",
    "\n",
    "# --- Stop Profiling ---\n",
    "print(\"Stopping profiling...\")\n",
    "pr.disable()\n",
    "print(\"Profiling stopped.\")\n",
    "\n",
    "# Process profiling results\n",
    "print(\"Processing profiling results...\")\n",
    "s = io.StringIO()\n",
    "sortby = 'cumulative' # Sort results by cumulative time\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats()\n",
    "profiling_output = s.getvalue()\n",
    "print(\"Profiling results processed.\")\n",
    "\n",
    "# Print a snippet of profiling results (optional)\n",
    "print(\"\\n--- Profiling Snippet (Top 10 by Cumulative Time) ---\")\n",
    "print('\\n'.join(profiling_output.splitlines()[:15])) # Print header and top few lines\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "\n",
    "# --- Cell 6: Model Evaluation and Metric Logging ---\n",
    "\n",
    "# Make predictions on the test set\n",
    "print(\"Making predictions on the test set...\")\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Predictions made.\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0) # Use weighted average for multi-class\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Metrics calculated.\")\n",
    "\n",
    "# Log metrics to MLflow and W&B\n",
    "print(\"Logging metrics to MLflow and W&B...\")\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "mlflow.log_metric(\"precision\", precision)\n",
    "mlflow.log_metric(\"recall\", recall)\n",
    "mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "wandb.log({\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1_score\": f1\n",
    "})\n",
    "print(\"Metrics logged.\")\n",
    "\n",
    "# --- Cell 7: Model and Artifact Logging ---\n",
    "\n",
    "# Log the trained model to MLflow\n",
    "print(\"Logging model with MLflow...\")\n",
    "# The model will be saved under the 'artifacts' directory of the MLflow run\n",
    "mlflow.sklearn.log_model(model, \"decision_tree_model\")\n",
    "print(\"Model logged to MLflow.\")\n",
    "\n",
    "# Log profiling results as an artifact to MLflow and W&B\n",
    "print(\"Logging profiling results as artifacts...\")\n",
    "profiling_output_filename = \"profiling_results.txt\"\n",
    "with open(profiling_output_filename, \"w\") as f:\n",
    "    f.write(profiling_output)\n",
    "\n",
    "mlflow.log_artifact(profiling_output_filename)\n",
    "wandb.save(profiling_output_filename)\n",
    "\n",
    "print(f\"Profiling results logged as artifact: {profiling_output_filename}\")\n",
    "\n",
    "# Clean up the temporary profiling file\n",
    "os.remove(profiling_output_filename)\n",
    "print(f\"Temporary profiling file removed: {profiling_output_filename}\")\n",
    "\n",
    "\n",
    "# --- Cell 8: End Runs ---\n",
    "\n",
    "# End the MLflow run\n",
    "print(\"Ending MLflow run...\")\n",
    "mlflow.end_run()\n",
    "print(\"MLflow run ended.\")\n",
    "\n",
    "# End the W&B run\n",
    "print(\"Ending W&B run...\")\n",
    "wandb.finish()\n",
    "print(\"W&B run finished.\")\n",
    "\n",
    "print(\"\\nExperiment complete. Check MLflow UI at http://localhost:5000 and W&B UI at http://localhost:8082\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa828c-968c-40c2-b242-e8d899a1cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cProfile, io, pstats\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "os.environ[\"WANDB_BASE_URL\"] = \"http://wandb:8080\"\n",
    "os.environ[\"WANDB_DEBUG\"] = \"true\"\n",
    "os.environ[\"WANDB_DEBUG_LOG_PATH\"] = \"/tmp/wandb_debug.log\"\n",
    "import wandb\n",
    "from wandb.sdk.wandb_settings import Settings\n",
    "\n",
    "def setup_tracking(dataset_name: str, model_name: str, config: dict):\n",
    "    # MLflow\n",
    "    experiment_name = f\"{dataset_name}_{model_name}\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    if mlflow.active_run() is None:\n",
    "        mlflow.start_run()\n",
    "        print(f\"MLflow run started: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "    for k, v in config.items():\n",
    "        mlflow.log_param(k, v)\n",
    "\n",
    "    # W&B\n",
    "    wandb_project_name = f\"kaggle_{dataset_name}\"\n",
    "    wandb_run_name = f\"{model_name}-training\"\n",
    "    wandb_run = wandb.init(\n",
    "        project=wandb_project_name,\n",
    "        name=wandb_run_name,\n",
    "        config=config,\n",
    "        settings=Settings(init_timeout=60)\n",
    "    )\n",
    "    return wandb_run\n",
    "\n",
    "def evaluate_and_log(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    }\n",
    "\n",
    "    print(\"Evaluation metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "        mlflow.log_metric(k, v)\n",
    "        wandb.log({k: v})\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate_and_log(model, X_test, y_test, task_type=None):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Try to infer task type if not provided\n",
    "    if task_type is None:\n",
    "        if len(np.unique(y_test)) > 12 and y_test.dtype.kind in \"if\":\n",
    "            task_type = \"regression\"\n",
    "        else:\n",
    "            task_type = \"classification\"\n",
    "\n",
    "    metrics = {}\n",
    "    if task_type == \"classification\":\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "            \"recall\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "            \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "        }\n",
    "    elif task_type == \"regression\":\n",
    "        metrics = {\n",
    "            \"mse\": mean_squared_error(y_test, y_pred),\n",
    "            \"mae\": mean_absolute_error(y_test, y_pred),\n",
    "            \"r2\": r2_score(y_test, y_pred)\n",
    "        }\n",
    "    elif task_type == \"llm\":\n",
    "        # Placeholder for LLM-style task evaluation\n",
    "        metrics = {\n",
    "            \"bleu\": 1, #your_bleu_fn(y_test, y_pred),\n",
    "            \"rouge\": 1, #your_rouge_fn(y_test, y_pred),\n",
    "            \"BERTScore\" : 1,\n",
    "            # Add more as needed\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "\n",
    "    print(f\"Task type: {task_type}\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "        mlflow.log_metric(k, v)\n",
    "        wandb.log({k: v})\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def profile_and_log(func, filename=\"profiling_results.txt\"):\n",
    "    pr = cProfile.Profile()\n",
    "    pr.enable()\n",
    "    func()\n",
    "    pr.disable()\n",
    "    s = io.StringIO()\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')\n",
    "    ps.print_stats()\n",
    "    profiling_output = s.getvalue()\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(profiling_output)\n",
    "\n",
    "    mlflow.log_artifact(filename)\n",
    "    wandb.save(filename)\n",
    "    os.remove(filename)\n",
    "\n",
    "def run_experiment(\n",
    "    model_class: BaseEstimator,\n",
    "    model_params: dict,\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    dataset_name=\"red-wine-quality-cortez-et-al-2009\",\n",
    "    etl_version=\"v1\"\n",
    "):\n",
    "    # Add ETL version and other metadata\n",
    "    config = model_params.copy()\n",
    "    config[\"etl_version\"] = etl_version\n",
    "\n",
    "    wandb_run = setup_tracking(dataset_name, model_class.__name__, config)\n",
    "\n",
    "    def train_model():\n",
    "        print(\"Training model...\")\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Model trained.\")\n",
    "        evaluate_and_log(model, X_test, y_test, task_type=\"classification\")\n",
    "        mlflow.sklearn.log_model(model, f\"{model_class.__name__}_model\")\n",
    "\n",
    "    profile_and_log(train_model)\n",
    "\n",
    "    # Finish runs\n",
    "    mlflow.end_run()\n",
    "    wandb.finish()\n",
    "    print(\"Experiment complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56217f92-cbdf-4e64-9649-43402b71b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "run_experiment(\n",
    "    model_class=DecisionTreeClassifier,\n",
    "    model_params={\"max_depth\": 10, \"random_state\": 42},\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    etl_version=\"2025-05-cleaned\"\n",
    ")\n",
    "\n",
    "run_experiment(\n",
    "    model_class=RandomForestClassifier,\n",
    "    model_params={\"n_estimators\": 100, \"max_depth\": 8, \"random_state\": 42},\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    etl_version=\"2025-05-cleaned\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
