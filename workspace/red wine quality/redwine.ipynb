{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5188c5b-ae79-40c8-a2ab-c76d910900a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dataset base path: /home/jovyan/data/red-wine-quality-cortez-et-al-2009\n",
      "Dataset not found locally. Downloading uciml/red-wine-quality-cortez-et-al-2009...\n",
      "Dataset downloaded to temporary path in container: /home/jovyan/.cache/kagglehub/datasets/uciml/red-wine-quality-cortez-et-al-2009/versions/2\n",
      "Copying dataset to persistent raw data volume: /home/jovyan/data/red-wine-quality-cortez-et-al-2009/raw\n",
      "Dataset 'uciml/red-wine-quality-cortez-et-al-2009' successfully copied to /home/jovyan/data/red-wine-quality-cortez-et-al-2009/raw in shared volume.\n",
      "Raw dataset saved to persistent volume at: /home/jovyan/data/red-wine-quality-cortez-et-al-2009/raw\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from utils.etl import run_new_etl \n",
    "\n",
    "dataset_id = \"uciml/red-wine-quality-cortez-et-al-2009\"\n",
    "local_dataset_name = dataset_id.split('/')[-1] # Uses the last part of the ID\n",
    "\n",
    "\n",
    "base_data_dir = Path(\"/home/jovyan/data\") # Standard for jovyan user\n",
    "# base_data_dir = project_root / \"data\" # Alternative for local non-Docker runs\n",
    "dataset_specific_base_path = base_data_dir / local_dataset_name\n",
    "raw_data_target_dir = dataset_specific_base_path / \"raw\"\n",
    "print(f\"Target dataset base path: {dataset_specific_base_path}\")\n",
    "\n",
    "# expected_raw_file_name = \"winequality-red.csv\"\n",
    "# final_raw_file_path = raw_data_target_dir / expected_raw_file_name\n",
    "# print(f\"Target raw data file: {final_raw_file_path}\")\n",
    "\n",
    "# Check if dataset already exists in destination\n",
    "destination_path = f\"/home/jovyan/data/{local_dataset_name}\"\n",
    "if os.path.exists(raw_data_target_dir) and os.listdir(dataset_specific_base_path):\n",
    "    print(f\"Raw dataset already exists at {raw_data_target_dir}\")\n",
    "    print(\"Skipping download...\")\n",
    "else:\n",
    "    print(f\"Dataset not found locally. Downloading {dataset_id}...\")\n",
    "    download_path = kagglehub.dataset_download(dataset_id)\n",
    "    print(f\"Dataset downloaded to temporary path in container: {download_path}\")\n",
    "    print(f\"Copying dataset to persistent raw data volume: {raw_data_target_dir}\")\n",
    "    # Ensure the destination directory exists \n",
    "    os.makedirs(raw_data_target_dir, exist_ok=True)\n",
    "    for item in os.listdir(download_path):\n",
    "        s = os.path.join(download_path, item)\n",
    "        d = os.path.join(raw_data_target_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, symlinks=False, ignore=None, dirs_exist_ok=True)\n",
    "        else:\n",
    "            shutil.copy2(s, d)\n",
    "\n",
    "    print(f\"Dataset '{dataset_id}' successfully copied to {raw_data_target_dir} in shared volume.\")\n",
    "print(f\"Raw dataset saved to persistent volume at: {raw_data_target_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ef36c95-81dc-45b5-a91a-6134692ce15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files in /home/jovyan/data/red-wine-quality-cortez-et-al-2009/raw:\n",
      "raw/\n",
      "    winequality-red.csv\n",
      "\n",
      "Loading data from: /home/jovyan/data/red-wine-quality-cortez-et-al-2009/raw/winequality-red.csv\n",
      "\n",
      "Available columns in the dataset:\n",
      "0: fixed acidity\n",
      "1: volatile acidity\n",
      "2: citric acid\n",
      "3: residual sugar\n",
      "4: chlorides\n",
      "5: free sulfur dioxide\n",
      "6: total sulfur dioxide\n",
      "7: density\n",
      "8: pH\n",
      "9: sulphates\n",
      "10: alcohol\n",
      "11: quality\n",
      "\n",
      "Target column 'quality' has been saved.\n",
      "Dataset loaded successfully.\n",
      "Dataset shape: (1599, 12)\n",
      "Dataset columns: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "Dataset head:\n",
      "    fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n",
      "Features shape: (1599, 11)\n",
      "Target shape: (1599,)\n",
      "Data split into training (1279 samples) and testing (320 samples).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Import Libraries and Load Data ---\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os\n",
    "import cProfile # For basic profiling\n",
    "import pstats # For processing profiling results\n",
    "import io # For capturing profiling output\n",
    "\n",
    "print(f\"Listing files in {raw_data_target_dir}:\")\n",
    "try:\n",
    "    for root, dirs, files in os.walk(raw_data_target_dir):\n",
    "        level = root.replace(str(raw_data_target_dir), '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f'{indent}{os.path.basename(root)}/')\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f'{subindent}{f}')\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Directory not found: {raw_data_target_dir}. Please ensure the dataset was downloaded and copied correctly.\")\n",
    "    # Exit or handle the error appropriately if the directory is not found\n",
    "\n",
    "# Adjust this path based on the actual file name and structure\n",
    "# Find the first CSV file in the dataset directory\n",
    "\n",
    "csv_files = [f for f in os.listdir(raw_data_target_dir) if f.endswith('.csv')]\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {raw_data_target_dir}\")\n",
    "if len(csv_files) > 1:\n",
    "    print(f\"Warning: Multiple CSV files found. Using the first one: {csv_files[0]}\")\n",
    "\n",
    "# Load the data\n",
    "data_file_path = os.path.join(raw_data_target_dir, csv_files[0])\n",
    "print(f\"\\nLoading data from: {data_file_path}\")\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "# Display available columns and prompt for target\n",
    "print(\"\\nAvailable columns in the dataset:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i}: {col}\")\n",
    "\n",
    "# Create a flag file to check if target has been selected\n",
    "target_flag_file = os.path.join(dataset_path, '.target_selected')\n",
    "# target_column = 'quality'\n",
    "if not os.path.exists(target_flag_file):\n",
    "    print(\"\\nPlease set target_column variable above and run this cell again.\")\n",
    "    print(\"Example: target_column = 'column_name'\")\n",
    "    # Create an empty flag file to indicate we need target selection\n",
    "    with open(target_flag_file, 'w') as f:\n",
    "        pass\n",
    "    raise SystemExit(\"Waiting for target column selection...\")\n",
    "\n",
    "# If target_column is defined and valid, save it to the flag file\n",
    "try:\n",
    "    if target_column in df.columns:\n",
    "        with open(target_flag_file, 'w') as f:\n",
    "            f.write(target_column)\n",
    "        print(f\"\\nTarget column '{target_column}' has been saved.\")\n",
    "    else:\n",
    "        os.remove(target_flag_file)  # Remove flag file if target is invalid\n",
    "        raise ValueError(f\"Selected target column '{target_column}' not found in dataset columns\")\n",
    "except NameError:\n",
    "    os.remove(target_flag_file)  # Remove flag file if target_column not defined\n",
    "    raise NameError(\"target_column variable not defined. Please define it and run again.\")\n",
    "\n",
    "\n",
    "\n",
    "# data_file_path = os.path.join(dataset_path, 'winequality-red.csv') # ** ADJUST THIS **\n",
    "# Check if target_column is defined and valid\n",
    "try:\n",
    "    with open(target_flag_file, 'r') as f:\n",
    "        target_column = f.read().strip()\n",
    "    if not target_column:\n",
    "        raise ValueError(\"Target column not found in flag file\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(\"Dataset shape:\", df.shape)\n",
    "    print(\"Dataset columns:\", df.columns.tolist())\n",
    "    print(\"Dataset head:\\n\", df.head())\n",
    "\n",
    "    # Verify target column exists\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in the dataset.\")\n",
    "\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Data split into training ({X_train.shape[0]} samples) and testing ({X_test.shape[0]} samples).\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data file not found at {data_file_path}. Please check the file name and path.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error during data preparation: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during data loading or preparation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc7a0745-05b9-44a8-a82d-6be6d2f23462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting MLflow experiment: red-wine-quality-cortez-et-al-2009_Decision_Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: http://wandb:8080/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow run already active with ID: 040dca790add4ad4b16591e41ad8e0fb\n",
      "Get wandb API key from localhost:8082/authorize\n",
      "Initializing W&B run: Project='kaggle_red-wine-quality-cortez-et-al-2009', Name='decision-tree-training'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for host.docker.internal:8082 to your netrc file: /home/jovyan/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged in\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/wandb/run-20250514_181906-yeb8ztxn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009/runs/yeb8ztxn' target=\"_blank\">decision-tree-training</a></strong> to <a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009' target=\"_blank\">http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009/runs/yeb8ztxn' target=\"_blank\">http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009/runs/yeb8ztxn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started W&B run with ID: yeb8ztxn\n",
      "Logging parameters to MLflow and W&B...\n",
      "Parameters logged.\n",
      "Starting profiling...\n",
      "Training Decision Tree model...\n",
      "Model training complete.\n",
      "Stopping profiling...\n",
      "Profiling stopped.\n",
      "Processing profiling results...\n",
      "Profiling results processed.\n",
      "\n",
      "--- Profiling Snippet (Top 10 by Cumulative Time) ---\n",
      "         3939 function calls (3889 primitive calls) in 0.009 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        6    0.000    0.000    0.009    0.002 /opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3490(run_code)\n",
      "        6    0.000    0.000    0.009    0.002 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.009    0.009 /opt/conda/lib/python3.11/site-packages/sklearn/base.py:1372(wrapper)\n",
      "        1    0.000    0.000    0.008    0.008 /opt/conda/lib/python3.11/site-packages/sklearn/tree/_classes.py:993(fit)\n",
      "        1    0.006    0.006    0.008    0.008 /opt/conda/lib/python3.11/site-packages/sklearn/tree/_classes.py:231(_fit)\n",
      "        1    0.000    0.000    0.002    0.002 /opt/conda/lib/python3.11/site-packages/sklearn/utils/validation.py:2835(validate_data)\n",
      "        4    0.000    0.000    0.002    0.000 /opt/conda/lib/python3.11/site-packages/sklearn/utils/validation.py:736(check_array)\n",
      "        8    0.000    0.000    0.001    0.000 /opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:6432(dtypes)\n",
      "        9    0.000    0.000    0.001    0.000 /opt/conda/lib/python3.11/site-packages/pandas/core/series.py:389(__init__)\n",
      "      145    0.000    0.000    0.001    0.000 {built-in method builtins.hasattr}\n",
      "----------------------------------------------------\n",
      "Making predictions on the test set...\n",
      "Predictions made.\n",
      "Calculating evaluation metrics...\n",
      "Accuracy: 0.5531\n",
      "Precision: 0.5320\n",
      "Recall: 0.5531\n",
      "F1 Score: 0.5409\n",
      "Metrics calculated.\n",
      "Logging metrics to MLflow and W&B...\n",
      "Metrics logged.\n",
      "Logging model with MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/14 18:19:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged to MLflow.\n",
      "Logging profiling results as artifacts...\n",
      "Profiling results logged as artifact: profiling_results.txt\n",
      "Temporary profiling file removed: profiling_results.txt\n",
      "Ending MLflow run...\n",
      "üèÉ View run omniscient-conch-714 at: http://mlflow:5000/#/experiments/212578242154088023/runs/040dca790add4ad4b16591e41ad8e0fb\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/212578242154088023\n",
      "MLflow run ended.\n",
      "Ending W&B run...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>f1_score</td><td>‚ñÅ</td></tr><tr><td>precision</td><td>‚ñÅ</td></tr><tr><td>recall</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.55312</td></tr><tr><td>f1_score</td><td>0.5409</td></tr><tr><td>precision</td><td>0.53197</td></tr><tr><td>recall</td><td>0.55312</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decision-tree-training</strong> at: <a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009/runs/yeb8ztxn' target=\"_blank\">http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009/runs/yeb8ztxn</a><br> View project at: <a href='http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009' target=\"_blank\">http://host.docker.internal:8082/ben/kaggle_red-wine-quality-cortez-et-al-2009</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/home/jovyan/wandb/wandb/run-20250514_181906-yeb8ztxn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run finished.\n",
      "\n",
      "Experiment complete. Check MLflow UI at http://localhost:5000 and W&B UI at http://localhost:8082\n"
     ]
    }
   ],
   "source": [
    "# --- MLflow and W&B Setup, Profiling Start ---\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "# os.environ[\"WANDB_BASE_URL\"] = \"http://host.docker.internal:8082\"\n",
    "os.environ[\"WANDB_BASE_URL\"] = \"http://wandb:8080\"\n",
    "os.environ[\"WANDB_DEBUG\"] = \"true\"\n",
    "os.environ[\"WANDB_DEBUG_LOG_PATH\"] = \"/tmp/wandb_debug.log\"\n",
    "import wandb\n",
    "# Ensure MLflow tracking URI is set (should be from environment variable)\n",
    "# mlflow.set_tracking_uri(\"http://mlflow:5000\") # This should be set by docker-compose env var\n",
    "\n",
    "# Set the MLflow experiment name\n",
    "mlflow_experiment_name = f\"{local_dataset_name}_Decision_Tree\"\n",
    "print(f\"\\nSetting MLflow experiment: {mlflow_experiment_name}\")\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "mlflow.log_param(\"dataset\", local_dataset_name)\n",
    "mlflow.log_param(\"dataset_description\", \"Red wine quality dataset from Cortez et al., 2009\")\n",
    "mlflow.log_param(\"dataset_version\", \"v1.0\")\n",
    "\n",
    "# Only start a new run if none is active\n",
    "active_run = mlflow.active_run()\n",
    "if active_run is None:\n",
    "    mlflow_run = mlflow.start_run()\n",
    "    print(f\"Started new MLflow run with ID: {mlflow_run.info.run_id}\")\n",
    "else:\n",
    "    print(f\"MLflow run already active with ID: {active_run.info.run_id}\")\n",
    "    \n",
    "\n",
    "# W&B: Initialize a new run\n",
    "# The project name helps organize runs in the W&B UI\n",
    "# The WANDB_DIR environment variable in docker-compose.yml ensures data goes to the shared volume\n",
    "wandb_project_name = f\"kaggle_{local_dataset_name}\"\n",
    "wandb_run_name = \"decision-tree-training\"\n",
    "print(\"Get wandb API key from localhost:8082/authorize\")\n",
    "print(f\"Initializing W&B run: Project='{wandb_project_name}', Name='{wandb_run_name}'\")\n",
    "wandb.login(relogin=True, host=\"http://host.docker.internal:8082\")\n",
    "print(\"logged in\")\n",
    "wandb.init(project=wandb_project_name, name=wandb_run_name)\n",
    "print(f\"Started W&B run with ID: {wandb.run.id}\")\n",
    "\n",
    "# Define model parameters\n",
    "max_depth = 4 # Example hyperparameter\n",
    "random_state = 20\n",
    "\n",
    "# Log parameters to MLflow and W&B\n",
    "print(\"Logging parameters to MLflow and W&B...\")\n",
    "mlflow.log_param(\"max_depth\", max_depth)\n",
    "mlflow.log_param(\"random_state\", random_state)\n",
    "wandb.config.max_depth = max_depth\n",
    "wandb.config.random_state = random_state\n",
    "print(\"Parameters logged.\")\n",
    "\n",
    "# --- Start Profiling ---\n",
    "# Profiling the training process to understand where time is spent\n",
    "print(\"Starting profiling...\")\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "\n",
    "\n",
    "# --- Cell 4: Model Training ---\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "print(\"Training Decision Tree model...\")\n",
    "model = DecisionTreeClassifier(max_depth=max_depth, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "\n",
    "# --- Cell 5: Profiling Stop and Processing ---\n",
    "\n",
    "# --- Stop Profiling ---\n",
    "print(\"Stopping profiling...\")\n",
    "pr.disable()\n",
    "print(\"Profiling stopped.\")\n",
    "\n",
    "# Process profiling results\n",
    "print(\"Processing profiling results...\")\n",
    "s = io.StringIO()\n",
    "sortby = 'cumulative' # Sort results by cumulative time\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats()\n",
    "profiling_output = s.getvalue()\n",
    "print(\"Profiling results processed.\")\n",
    "\n",
    "# Print a snippet of profiling results (optional)\n",
    "print(\"\\n--- Profiling Snippet (Top 10 by Cumulative Time) ---\")\n",
    "print('\\n'.join(profiling_output.splitlines()[:15])) # Print header and top few lines\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "\n",
    "# --- Cell 6: Model Evaluation and Metric Logging ---\n",
    "\n",
    "# Make predictions on the test set\n",
    "print(\"Making predictions on the test set...\")\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Predictions made.\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0) # Use weighted average for multi-class\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Metrics calculated.\")\n",
    "\n",
    "# Log metrics to MLflow and W&B\n",
    "print(\"Logging metrics to MLflow and W&B...\")\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "mlflow.log_metric(\"precision\", precision)\n",
    "mlflow.log_metric(\"recall\", recall)\n",
    "mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "wandb.log({\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1_score\": f1\n",
    "})\n",
    "print(\"Metrics logged.\")\n",
    "\n",
    "# --- Cell 7: Model and Artifact Logging ---\n",
    "\n",
    "# Log the trained model to MLflow\n",
    "print(\"Logging model with MLflow...\")\n",
    "# The model will be saved under the 'artifacts' directory of the MLflow run\n",
    "mlflow.sklearn.log_model(model, \"decision_tree_model\")\n",
    "print(\"Model logged to MLflow.\")\n",
    "\n",
    "# Log profiling results as an artifact to MLflow and W&B\n",
    "print(\"Logging profiling results as artifacts...\")\n",
    "profiling_output_filename = \"profiling_results.txt\"\n",
    "with open(profiling_output_filename, \"w\") as f:\n",
    "    f.write(profiling_output)\n",
    "\n",
    "mlflow.log_artifact(profiling_output_filename)\n",
    "wandb.save(profiling_output_filename)\n",
    "\n",
    "print(f\"Profiling results logged as artifact: {profiling_output_filename}\")\n",
    "\n",
    "# Clean up the temporary profiling file\n",
    "os.remove(profiling_output_filename)\n",
    "print(f\"Temporary profiling file removed: {profiling_output_filename}\")\n",
    "\n",
    "\n",
    "# --- Cell 8: End Runs ---\n",
    "\n",
    "# End the MLflow run\n",
    "print(\"Ending MLflow run...\")\n",
    "mlflow.end_run()\n",
    "print(\"MLflow run ended.\")\n",
    "\n",
    "# End the W&B run\n",
    "print(\"Ending W&B run...\")\n",
    "wandb.finish()\n",
    "print(\"W&B run finished.\")\n",
    "\n",
    "print(\"\\nExperiment complete. Check MLflow UI at http://localhost:5000 and W&B UI at http://localhost:8082\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa828c-968c-40c2-b242-e8d899a1cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cProfile, io, pstats\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "os.environ[\"WANDB_BASE_URL\"] = \"http://wandb:8080\"\n",
    "os.environ[\"WANDB_DEBUG\"] = \"true\"\n",
    "os.environ[\"WANDB_DEBUG_LOG_PATH\"] = \"/tmp/wandb_debug.log\"\n",
    "import wandb\n",
    "from wandb.sdk.wandb_settings import Settings\n",
    "\n",
    "def setup_tracking(dataset_name: str, model_name: str, config: dict):\n",
    "    # MLflow\n",
    "    experiment_name = f\"{dataset_name}_{model_name}\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    if mlflow.active_run() is None:\n",
    "        mlflow.start_run()\n",
    "        print(f\"MLflow run started: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "    for k, v in config.items():\n",
    "        mlflow.log_param(k, v)\n",
    "\n",
    "    # W&B\n",
    "    wandb_project_name = f\"kaggle_{dataset_name}\"\n",
    "    wandb_run_name = f\"{model_name}-training\"\n",
    "    wandb_run = wandb.init(\n",
    "        project=wandb_project_name,\n",
    "        name=wandb_run_name,\n",
    "        config=config,\n",
    "        settings=Settings(init_timeout=60)\n",
    "    )\n",
    "    return wandb_run\n",
    "\n",
    "def evaluate_and_log(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    }\n",
    "\n",
    "    print(\"Evaluation metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "        mlflow.log_metric(k, v)\n",
    "        wandb.log({k: v})\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate_and_log(model, X_test, y_test, task_type=None):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Try to infer task type if not provided\n",
    "    if task_type is None:\n",
    "        if len(np.unique(y_test)) > 12 and y_test.dtype.kind in \"if\":\n",
    "            task_type = \"regression\"\n",
    "        else:\n",
    "            task_type = \"classification\"\n",
    "\n",
    "    metrics = {}\n",
    "    if task_type == \"classification\":\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "            \"recall\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "            \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "        }\n",
    "    elif task_type == \"regression\":\n",
    "        metrics = {\n",
    "            \"mse\": mean_squared_error(y_test, y_pred),\n",
    "            \"mae\": mean_absolute_error(y_test, y_pred),\n",
    "            \"r2\": r2_score(y_test, y_pred)\n",
    "        }\n",
    "    elif task_type == \"llm\":\n",
    "        # Placeholder for LLM-style task evaluation\n",
    "        metrics = {\n",
    "            \"bleu\": 1, #your_bleu_fn(y_test, y_pred),\n",
    "            \"rouge\": 1, #your_rouge_fn(y_test, y_pred),\n",
    "            \"BERTScore\" : 1,\n",
    "            # Add more as needed\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "\n",
    "    print(f\"Task type: {task_type}\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "        mlflow.log_metric(k, v)\n",
    "        wandb.log({k: v})\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def profile_and_log(func, filename=\"profiling_results.txt\"):\n",
    "    pr = cProfile.Profile()\n",
    "    pr.enable()\n",
    "    func()\n",
    "    pr.disable()\n",
    "    s = io.StringIO()\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')\n",
    "    ps.print_stats()\n",
    "    profiling_output = s.getvalue()\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(profiling_output)\n",
    "\n",
    "    mlflow.log_artifact(filename)\n",
    "    wandb.save(filename)\n",
    "    os.remove(filename)\n",
    "\n",
    "def run_experiment(\n",
    "    model_class: BaseEstimator,\n",
    "    model_params: dict,\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    dataset_name=\"red-wine-quality-cortez-et-al-2009\",\n",
    "    etl_version=\"v1\"\n",
    "):\n",
    "    # Add ETL version and other metadata\n",
    "    config = model_params.copy()\n",
    "    config[\"etl_version\"] = etl_version\n",
    "\n",
    "    wandb_run = setup_tracking(dataset_name, model_class.__name__, config)\n",
    "\n",
    "    def train_model():\n",
    "        print(\"Training model...\")\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Model trained.\")\n",
    "        evaluate_and_log(model, X_test, y_test, task_type=\"classification\")\n",
    "        mlflow.sklearn.log_model(model, f\"{model_class.__name__}_model\")\n",
    "\n",
    "    profile_and_log(train_model)\n",
    "\n",
    "    # Finish runs\n",
    "    mlflow.end_run()\n",
    "    wandb.finish()\n",
    "    print(\"Experiment complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56217f92-cbdf-4e64-9649-43402b71b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "run_experiment(\n",
    "    model_class=DecisionTreeClassifier,\n",
    "    model_params={\"max_depth\": 10, \"random_state\": 42},\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    etl_version=\"2025-05-cleaned\"\n",
    ")\n",
    "\n",
    "run_experiment(\n",
    "    model_class=RandomForestClassifier,\n",
    "    model_params={\"n_estimators\": 100, \"max_depth\": 8, \"random_state\": 42},\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    etl_version=\"2025-05-cleaned\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
