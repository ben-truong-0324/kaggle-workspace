{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dataset base path: /home/jovyan/data/space-titanic\n",
      "Raw dataset already exists at /home/jovyan/data/space-titanic/raw\n",
      "Skipping download...\n",
      "Raw dataset in persistent volume at: /home/jovyan/data/space-titanic/raw\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# dataset_id = \"uciml/red-wine-quality-cortez-et-al-2009\"\n",
    "# local_dataset_name = dataset_id.split('/')[-1] # Uses the last part of the ID\n",
    "\n",
    "local_dataset_name = \"space-titanic\"\n",
    "\n",
    "base_data_dir = Path(\"/home/jovyan/data\") # Standard for jovyan user\n",
    "dataset_specific_base_path = base_data_dir / local_dataset_name\n",
    "raw_data_target_dir = dataset_specific_base_path / \"raw\"\n",
    "print(f\"Target dataset base path: {dataset_specific_base_path}\")\n",
    "os.makedirs(raw_data_target_dir, exist_ok=True)\n",
    "\n",
    "# Check if dataset already exists in destination\n",
    "destination_path = f\"/home/jovyan/data/{local_dataset_name}\"\n",
    "if os.path.exists(raw_data_target_dir) and os.listdir(dataset_specific_base_path):\n",
    "    print(f\"Raw dataset already exists at {raw_data_target_dir}\")\n",
    "    print(\"Skipping download...\")\n",
    "else:\n",
    "    print(f\"Dataset not found locally. Downloading {dataset_id}...\")\n",
    "    download_path = kagglehub.dataset_download(dataset_id)\n",
    "    print(f\"Dataset downloaded to temporary path in container: {download_path}\")\n",
    "    print(f\"Copying dataset to persistent raw data volume: {raw_data_target_dir}\")\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(raw_data_target_dir, exist_ok=True)\n",
    "    for item in os.listdir(download_path):\n",
    "        s = os.path.join(download_path, item)\n",
    "        d = os.path.join(raw_data_target_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, symlinks=False, ignore=None, dirs_exist_ok=True)\n",
    "        else:\n",
    "            shutil.copy2(s, d)\n",
    "\n",
    "    print(f\"Dataset '{dataset_id}' successfully copied to {raw_data_target_dir} in shared volume.\")\n",
    "print(f\"Raw dataset in persistent volume at: {raw_data_target_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files in /home/jovyan/data/space-titanic/raw:\n",
      "raw/\n",
      "    train.csv\n",
      "\n",
      "Loading data from: /home/jovyan/data/space-titanic/raw/train.csv\n",
      "\n",
      "Available columns in the RAW dataset:\n",
      "0: PassengerId\n",
      "1: HomePlanet\n",
      "2: CryoSleep\n",
      "3: Cabin\n",
      "4: Destination\n",
      "5: Age\n",
      "6: VIP\n",
      "7: RoomService\n",
      "8: FoodCourt\n",
      "9: ShoppingMall\n",
      "10: Spa\n",
      "11: VRDeck\n",
      "12: Name\n",
      "13: Transported\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Import Libraries and Load Data ---\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os\n",
    "import cProfile # For basic profiling\n",
    "import pstats # For processing profiling results\n",
    "import io # For capturing profiling output\n",
    "\n",
    "def reload_utils():\n",
    "    import importlib\n",
    "    import utils.etl\n",
    "    import red_wine_quality.etl_chain\n",
    "    import utils.eda\n",
    "    import utils.eval\n",
    "    import utils.submission\n",
    "\n",
    "    importlib.reload(utils.etl)\n",
    "    importlib.reload(red_wine_quality.etl_chain)\n",
    "    importlib.reload(utils.eda)\n",
    "    importlib.reload(utils.eval)\n",
    "    importlib.reload(utils.submission)\n",
    "    print(\"ðŸ” Reloaded red_wine_quality.etl_chain, utils.etl, utils.eda, utils.eval, utils.submission\")\n",
    "\n",
    "print(f\"Listing files in {raw_data_target_dir}:\")\n",
    "try:\n",
    "    for root, dirs, files in os.walk(raw_data_target_dir):\n",
    "        level = root.replace(str(raw_data_target_dir), '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f'{indent}{os.path.basename(root)}/')\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f'{subindent}{f}')\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Directory not found: {raw_data_target_dir}. Please ensure the dataset was downloaded and copied correctly.\")\n",
    "    # Exit or handle the error appropriately if the directory is not found\n",
    "\n",
    "csv_files = [f for f in os.listdir(raw_data_target_dir) if f.endswith('.csv')]\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {raw_data_target_dir}\")\n",
    "if len(csv_files) > 1:\n",
    "    print(f\"Warning: Multiple CSV files found. Using the first one: {csv_files[0]}\")\n",
    "\n",
    "# Load the data\n",
    "data_file_path = os.path.join(raw_data_target_dir, csv_files[0])\n",
    "print(f\"\\nLoading data from: {data_file_path}\")\n",
    "df = pd.read_csv(data_file_path)\n",
    "\n",
    "print(\"\\nAvailable columns in the RAW dataset:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i}: {col}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8693, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.describe()\n",
    "# df.columns\n",
    "# df.dtypes\n",
    "# df.isna().sum()\n",
    "# df.loc[df.duplicated()]\n",
    "# df.duplicated().sum()\n",
    "# df.loc[df.duplicated(subset=['citric acid'])].head(5)\n",
    "# df = df.loc[~df.duplicated(subset=['citric acid'])] \\\n",
    "#     .reset_index(drop=True).copy()\n",
    "df.shape\n",
    "# df.head()\n",
    "# df[\"HomePlanet\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§® 6606/8693 rows would remain after -2087 rows dropna (75.99%)\n"
     ]
    }
   ],
   "source": [
    "n_before = df.shape[0]\n",
    "n_after = df.dropna().shape[0]\n",
    "print(f\"ðŸ§® {n_after}/{n_before} rows would remain after {n_after - n_before} rows dropna ({100 * n_after/n_before:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/jovyan/data/space-titanic/raw/train.csv')]\n",
      "Dataset loaded successfully. Only one file loaded\n",
      "ðŸ§  Target column: Transported\n",
      "ðŸ“Š Target dtype: bool\n",
      "âœ… Inferred task type: binary_classification\n"
     ]
    }
   ],
   "source": [
    "# === setting pred target ===\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "from utils.etl import get_raw_dataset, transform_raw_dataframe\n",
    "import numpy as np\n",
    "target_column = \"Transported\"\n",
    "service_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "X, y = get_raw_dataset(dataset_name=local_dataset_name, target_column=target_column, drop_na = True)\n",
    "X = transform_raw_dataframe(df = X, service_cols= service_cols)\n",
    "# === Task Type Inference ===\n",
    "def detect_task_type(y):\n",
    "    if pd.api.types.is_numeric_dtype(y):\n",
    "        if y.nunique() <= 2:\n",
    "            return \"binary_classification\"\n",
    "        elif y.nunique() < 15: #arbitrary 15\n",
    "            return \"multiclass_classification\"\n",
    "        else:\n",
    "            return \"regression\"\n",
    "    elif y.apply(lambda x: isinstance(x, (list, set))).any():\n",
    "        return \"multilabel_classification\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "task_type = detect_task_type(y)\n",
    "\n",
    "print(f\"ðŸ§  Target column: {target_column}\")\n",
    "print(f\"ðŸ“Š Target dtype: {df[target_column].dtype}\")\n",
    "print(f\"âœ… Inferred task type: {task_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation metric set to: accuracy\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "perf_eval_metric = \"accuracy\"  # or \"accuracy\", \"precision\", \"recall\", etc.\n",
    "\n",
    "# Map of string -> (metric name, metric function)\n",
    "metric_lookup = {\n",
    "    \"accuracy\": (\"accuracy\", sklearn.metrics.accuracy_score),\n",
    "    \"f1\": (\"f1_score\", lambda y_true, y_pred: sklearn.metrics.f1_score(y_true, y_pred, average=\"weighted\")),\n",
    "    \"precision\": (\"precision\", lambda y_true, y_pred: sklearn.metrics.precision_score(y_true, y_pred, average=\"weighted\")),\n",
    "    \"recall\": (\"recall\", lambda y_true, y_pred: sklearn.metrics.recall_score(y_true, y_pred, average=\"weighted\")),\n",
    "    \"mse\": (\"mean_squared_error\", sklearn.metrics.mean_squared_error),\n",
    "    \"mae\": (\"mean_absolute_error\", sklearn.metrics.mean_absolute_error),\n",
    "}\n",
    "\n",
    "# Lookup the function and name\n",
    "if perf_eval_metric not in metric_lookup:\n",
    "    raise ValueError(f\"Unsupported metric: {perf_eval_metric}\")\n",
    "\n",
    "eval_metric_name, eval_metric_fn = metric_lookup[perf_eval_metric]\n",
    "print(f\"âœ… Evaluation metric set to: {eval_metric_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Domain knowledge ===\n",
    "# Any semantic/description of features?\n",
    "# PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n",
    "# HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n",
    "# CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
    "# Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
    "# Destination - The planet the passenger will be debarking to.\n",
    "# Age - The age of the passenger.\n",
    "# VIP - Whether the passenger has paid for special VIP service during the voyage.\n",
    "# RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
    "# Name - The first and last names of the passenger.\n",
    "# Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.eda import eda_vis\n",
    "# # === prelim EDA on raw dataset ===\n",
    "# eda_vis(X, y, task_type)\n",
    "# reload_utils()\n",
    "# from utils.eda import feature_eda_vis\n",
    "# # === deep EDA for features on ETL-applied dataset ===\n",
    "# feature_eda_vis(X, y, task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "# from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "# # Assume X and y are pandas DataFrames/Series as described by the user.\n",
    "# # For demonstration, let's create some sample data if X and y are not defined\n",
    "# # (Remove or comment this out if you have your actual X and y)\n",
    "# if 'X' not in locals() or 'y' not in locals():\n",
    "#     print(\"Generating sample X and y for demonstration purposes...\")\n",
    "#     n_samples = 200\n",
    "#     X_data = {\n",
    "#         'HomePlanet': np.random.choice(['Earth', 'Europa', 'Mars', 'PlanetX', 'PlanetY'], size=n_samples), # Added more categories\n",
    "#         'CryoSleep': np.random.choice([True, False, np.nan], size=n_samples, p=[0.25, 0.7, 0.05]), # Added NaNs\n",
    "#         'Age': np.random.normal(loc=30, scale=10, size=n_samples).clip(0, 80),\n",
    "#         'RoomService': np.random.exponential(scale=100, size=n_samples).clip(0, 2000) * np.random.choice([0,1, np.nan], size=n_samples, p=[0.35,0.55, 0.1]), # Some zeros and NaNs\n",
    "#         'VIP': np.random.choice([True, False, np.nan], size=n_samples, p=[0.05, 0.85, 0.1]) # With NaNs\n",
    "#     }\n",
    "#     X = pd.DataFrame(X_data)\n",
    "#     X['Age'] = X['Age'].astype(float).fillna(X['Age'].median()).astype(int) # Handle potential NaNs from clip then fill\n",
    "#     X['RoomService'] = X['RoomService'].astype(float)\n",
    "    \n",
    "#     # Simulate some dependency for y\n",
    "#     y_score = X['Age'] * -0.1 + X['RoomService'].fillna(0) * 0.01 + \\\n",
    "#               (X['HomePlanet'] == 'Europa').astype(int) * 20 + \\\n",
    "#               X['CryoSleep'].fillna(False).astype(int) * 30 # Handle NaNs in CryoSleep for scoring\n",
    "#     y_prob = 1 / (1 + np.exp(- (y_score - y_score.mean()) / y_score.std() )) # Sigmoid\n",
    "#     y = pd.Series(np.random.binomial(1, y_prob, size=n_samples).astype(bool), name='Transported')\n",
    "\n",
    "\n",
    "# # --- Data Preparation ---\n",
    "# y_named = y.copy()\n",
    "# if not hasattr(y_named, 'name') or y_named.name is None:\n",
    "#     y_named.name = 'Transported' # Default name if y has no name\n",
    "\n",
    "# df_combined = X.copy()\n",
    "# df_combined['Transported_numeric'] = y_named.astype(int)\n",
    "\n",
    "# CATEGORICAL_THRESHOLD = 20 \n",
    "# palette = {0: 'skyblue', 1: 'salmon'} \n",
    "# legend_labels = {0: 'Not Transported', 1: 'Transported'}\n",
    "# feature_columns = X.columns\n",
    "\n",
    "# # --- Main Loop for Plotting ---\n",
    "# for feature_col in feature_columns:\n",
    "#     print(f\"--- Analyzing Feature: {feature_col} ---\")\n",
    "    \n",
    "#     fig, axes = plt.subplots(2, 3, figsize=(22, 13)) # Adjusted figsize slightly for better label spacing\n",
    "#     fig.suptitle(f'Comprehensive Analysis: {feature_col} | Target: {y_named.name}', fontsize=18, y=0.99) # Adjusted y for suptitle\n",
    "    \n",
    "#     # --- Row 1: Distribution Visualizations ---\n",
    "#     ax_1_1 = axes[0, 0]\n",
    "#     try:\n",
    "#         plot_data_1_1 = df_combined[[feature_col, 'Transported_numeric']].dropna(subset=[feature_col])\n",
    "#         if not plot_data_1_1.empty and plot_data_1_1[feature_col].nunique() > 0 :\n",
    "#             sns.stripplot(data=plot_data_1_1, x=feature_col, y='Transported_numeric', hue='Transported_numeric',\n",
    "#                           jitter=0.25, dodge=True, ax=ax_1_1, palette=palette, legend=False, alpha=0.6)\n",
    "#             ax_1_1.set_title('Point Distribution by Target')\n",
    "#             ax_1_1.set_ylabel(f'{y_named.name} (0=F, 1=T)')\n",
    "#             ax_1_1.set_yticks([0, 1])\n",
    "#             ax_1_1.set_yticklabels(['False', 'True'])\n",
    "#             ax_1_1.set_xlabel(feature_col)\n",
    "#             handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=palette[i], markersize=10) for i in palette]\n",
    "#             ax_1_1.legend(handles, [legend_labels[i] for i in palette], title=y_named.name, loc='best')\n",
    "            \n",
    "#             # Rotate x-axis labels if needed\n",
    "#             if plot_data_1_1[feature_col].nunique() > 5 and plot_data_1_1[feature_col].dtype == 'object': # More categories\n",
    "#                  ax_1_1.tick_params(axis='x', rotation=45, labelbottom=True)\n",
    "#                  plt.setp(ax_1_1.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
    "#             elif plot_data_1_1[feature_col].nunique() > 10 and plot_data_1_1[feature_col].dtype != 'object': # Numerical with many ticks\n",
    "#                  ax_1_1.tick_params(axis='x', rotation=30, labelbottom=True)\n",
    "#                  plt.setp(ax_1_1.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
    "#             else: # Fewer categories or numerical, no rotation or default handling\n",
    "#                  ax_1_1.tick_params(axis='x', labelbottom=True)\n",
    "\n",
    "#         else:\n",
    "#             ax_1_1.text(0.5, 0.5, \"No data or no variance\\nafter NaN drop\", ha='center', va='center', transform=ax_1_1.transAxes)\n",
    "#             ax_1_1.set_title('Point Distribution (No Data)')\n",
    "#     except Exception as e:\n",
    "#         ax_1_1.set_title('Point Distribution (Error)')\n",
    "#         ax_1_1.text(0.5, 0.5, f\"Plot failed: {e}\", ha='center', va='center', transform=ax_1_1.transAxes, wrap=True)\n",
    "#         print(f\"  Error in Plot 1.1 for {feature_col}: {e}\")\n",
    "\n",
    "#     ax_1_2 = axes[0, 1]\n",
    "#     try:\n",
    "#         plot_data_1_2 = df_combined[[feature_col, 'Transported_numeric']].dropna(subset=[feature_col])\n",
    "#         if not plot_data_1_2.empty and plot_data_1_2[feature_col].nunique() > 0:\n",
    "#             sns.histplot(data=plot_data_1_2, x=feature_col, hue='Transported_numeric', \n",
    "#                          multiple='layer', kde=False, ax=ax_1_2, palette=palette, \n",
    "#                          stat=\"density\", common_norm=False, alpha=0.6, legend=True)\n",
    "#             ax_1_2.set_title('Normalized Histogram by Target')\n",
    "#             ax_1_2.set_xlabel(feature_col)\n",
    "#             handles_hist, labels_hist = ax_1_2.get_legend_handles_labels()\n",
    "#             try: \n",
    "#                 labels_hist_descriptive = [legend_labels[int(float(l))] for l in labels_hist]\n",
    "#                 ax_1_2.legend(handles_hist, labels_hist_descriptive, title=y_named.name)\n",
    "#             except (ValueError, KeyError): \n",
    "#                  ax_1_2.legend(title=y_named.name)\n",
    "#         else:\n",
    "#             ax_1_2.text(0.5, 0.5, \"No data or no variance\\nafter NaN drop\", ha='center', va='center', transform=ax_1_2.transAxes)\n",
    "#             ax_1_2.set_title('Histogram (No Data)')\n",
    "#     except Exception as e:\n",
    "#         ax_1_2.set_title('Histogram (Error)')\n",
    "#         ax_1_2.text(0.5, 0.5, f\"Plot failed: {e}\", ha='center', va='center', transform=ax_1_2.transAxes, wrap=True)\n",
    "#         print(f\"  Error in Plot 1.2 for {feature_col}: {e}\")\n",
    "\n",
    "#     ax_1_3 = axes[0, 2]\n",
    "#     try:\n",
    "#         temp_df_combined = df_combined.dropna(subset=[feature_col]) \n",
    "#         if not temp_df_combined.empty and temp_df_combined[feature_col].nunique() > 0:\n",
    "#             is_categorical_like = temp_df_combined[feature_col].dtype == 'object' or \\\n",
    "#                                   temp_df_combined[feature_col].nunique() < CATEGORICAL_THRESHOLD\n",
    "            \n",
    "#             target_0_data = temp_df_combined[temp_df_combined['Transported_numeric'] == 0][feature_col]\n",
    "#             target_1_data = temp_df_combined[temp_df_combined['Transported_numeric'] == 1][feature_col]\n",
    "\n",
    "#             if not target_0_data.empty and not target_1_data.empty:\n",
    "#                 if is_categorical_like:\n",
    "#                     props_0 = target_0_data.value_counts(normalize=True)\n",
    "#                     props_1 = target_1_data.value_counts(normalize=True)\n",
    "#                     all_categories = sorted(list(set(props_0.index) | set(props_1.index)))\n",
    "#                     props_0 = props_0.reindex(all_categories, fill_value=0)\n",
    "#                     props_1 = props_1.reindex(all_categories, fill_value=0)\n",
    "#                     diff_props = props_1 - props_0\n",
    "#                     diff_props.plot(kind='bar', ax=ax_1_3, color=['tomato' if x < 0 else 'mediumseagreen' for x in diff_props.values]) # Removed rot here\n",
    "#                     ax_1_3.tick_params(axis='x', rotation=45) # Apply rotation\n",
    "#                     plt.setp(ax_1_3.get_xticklabels(), ha='right', rotation_mode='anchor') # Ensure alignment\n",
    "#                     ax_1_3.set_ylabel('Prop(Y=1) - Prop(Y=0)')\n",
    "#                 else: \n",
    "#                     min_val = min(target_0_data.min(), target_1_data.min())\n",
    "#                     max_val = max(target_0_data.max(), target_1_data.max())\n",
    "#                     if pd.isna(min_val) or pd.isna(max_val) or min_val == max_val: # Handle NaN or single value case\n",
    "#                         bins = np.array([temp_df_combined[feature_col].min() - 0.5, temp_df_combined[feature_col].max() + 0.5]) if temp_df_combined[feature_col].nunique() > 0 else np.array([0,1])\n",
    "#                     else: \n",
    "#                         bins = np.linspace(min_val, max_val, 11)\n",
    "#                     hist_0, _ = np.histogram(target_0_data.dropna(), bins=bins, density=True) # dropna within histogram\n",
    "#                     hist_1, _ = np.histogram(target_1_data.dropna(), bins=bins, density=True) # dropna within histogram\n",
    "#                     diff_hist = hist_1 - hist_0\n",
    "#                     bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "#                     bar_width = bins[1] - bins[0] if len(bins) > 1 else 1\n",
    "#                     ax_1_3.bar(bin_centers, diff_hist, width=bar_width * 0.9, \n",
    "#                                 color=['tomato' if x < 0 else 'mediumseagreen' for x in diff_hist])\n",
    "#                     ax_1_3.set_ylabel('Density(Y=1) - Density(Y=0)')\n",
    "#                 ax_1_3.axhline(0, color='black', lw=0.8, linestyle='--')\n",
    "#                 ax_1_3.set_title('Outcome Difference by Feature Value')\n",
    "#                 ax_1_3.set_xlabel(feature_col)\n",
    "#             else:\n",
    "#                 ax_1_3.text(0.5, 0.5, \"Not enough data in one/both target groups\", ha='center', va='center', transform=ax_1_3.transAxes)\n",
    "#                 ax_1_3.set_title('Outcome Difference (Not enough data)')\n",
    "#         else:\n",
    "#             ax_1_3.text(0.5, 0.5, \"No data or no variance\\nafter NaN drop\", ha='center', va='center', transform=ax_1_3.transAxes)\n",
    "#             ax_1_3.set_title('Outcome Difference (No Data)')\n",
    "#     except Exception as e:\n",
    "#         ax_1_3.set_title('Outcome Difference (Error)')\n",
    "#         ax_1_3.text(0.5, 0.5, f\"Plot failed: {e}\", ha='center', va='center', transform=ax_1_3.transAxes, wrap=True)\n",
    "#         print(f\"  Error in Plot 1.3 for {feature_col}: {e}\")\n",
    "\n",
    "#     # --- Row 2: Statistical Significance ---\n",
    "#     ax_2_1 = axes[1, 0] \n",
    "#     ax_2_2 = axes[1, 1] \n",
    "#     ax_2_3 = axes[1, 2] \n",
    "\n",
    "#     for ax_text in [ax_2_2, ax_2_3]:\n",
    "#         ax_text.clear()\n",
    "#         ax_text.axis('off')\n",
    "\n",
    "#     current_feature_data = df_combined[[feature_col, 'Transported_numeric']].dropna(subset=[feature_col])\n",
    "#     if current_feature_data.empty or current_feature_data[feature_col].nunique() == 0: # Also check for no variance\n",
    "#         ax_2_1.text(0.5, 0.5, \"No data or no variance\\nfor stats after NaN drop\", ha='center', va='center', transform=ax_2_1.transAxes)\n",
    "#         ax_2_1.set_title('Stats (No Data)')\n",
    "#         ax_2_2.text(0.05, 0.9, \"No data or no variance for stats after NaN drop.\", fontsize=9, va='top', wrap=True)\n",
    "#         ax_2_3.text(0.05, 0.9, \"No data or no variance for stats after NaN drop.\", fontsize=9, va='top', wrap=True)\n",
    "#         fig.subplots_adjust(hspace=0.6, wspace=0.35, top=0.93, bottom=0.08, left=0.05, right=0.97)\n",
    "#         plt.show()\n",
    "#         continue \n",
    "\n",
    "#     is_categorical_feature = current_feature_data[feature_col].dtype == 'object' or \\\n",
    "#                              current_feature_data[feature_col].nunique() < CATEGORICAL_THRESHOLD\n",
    "    \n",
    "#     stats_summary_text = \"\"\n",
    "#     interpretation_text = \"\"\n",
    "\n",
    "#     if is_categorical_feature:\n",
    "#         ax_2_1.set_title(f'Proportion Transported by {feature_col}\\n(with 95% CIs)')\n",
    "#         # Ensure feature_col is treated as string for crosstab if it's not already object (e.g. boolean, int categories)\n",
    "#         contingency_table = pd.crosstab(current_feature_data[feature_col].astype(str), current_feature_data['Transported_numeric'])\n",
    "        \n",
    "#         if contingency_table.shape[0] < 1 or contingency_table.shape[1] < 2 : # Need at least 1 category and 2 outcome classes\n",
    "#             stats_summary_text += \"Chi-squared test not applicable (table too small or one outcome class missing).\\n\"\n",
    "#             ax_2_1.text(0.5,0.5, \"Too few categories or\\noutcomes for plot/test\", ha='center', va='center', transform=ax_2_1.transAxes)\n",
    "#         elif contingency_table.shape[0] < 2: # Need at least 2 categories for chi2\n",
    "#              stats_summary_text += \"Chi-squared test not applicable (needs at least 2 categories).\\n\"\n",
    "#              ax_2_1.text(0.5,0.5, \"Needs at least 2 categories for Chi2 test\", ha='center', va='center', transform=ax_2_1.transAxes)\n",
    "#         else:\n",
    "#             chi2, p_chi2, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "#             stats_summary_text += f\"Chi-squared Test of Independence:\\n\"\n",
    "#             stats_summary_text += f\"  Chi2 Stat: {chi2:.2f}, P-value: {p_chi2:.3g}\\n  DOF: {dof}\\n\"\n",
    "#             interpretation_text += f\"P-value ({p_chi2:.3g}) for Chi-squared test: \"\n",
    "#             interpretation_text += \"Suggests \" + (\"a significant\" if p_chi2 < 0.05 else \"no significant\") + \\\n",
    "#                                    f\" association between {feature_col} and {y_named.name}.\\n\\n\"\n",
    "\n",
    "#             categories = contingency_table.index\n",
    "#             proportions_transported = []\n",
    "#             ci_lows = []\n",
    "#             ci_highs = []\n",
    "            \n",
    "#             for cat in categories:\n",
    "#                 count_transported = contingency_table.loc[cat, 1] if 1 in contingency_table.columns else 0\n",
    "#                 count_not_transported = contingency_table.loc[cat, 0] if 0 in contingency_table.columns else 0\n",
    "#                 n_obs_cat = count_transported + count_not_transported\n",
    "#                 if n_obs_cat > 0:\n",
    "#                     prop = count_transported / n_obs_cat\n",
    "#                     low, high = proportion_confint(count_transported, n_obs_cat, method='wilson')\n",
    "#                     proportions_transported.append(prop)\n",
    "#                     ci_lows.append(low)\n",
    "#                     ci_highs.append(high)\n",
    "#                 else: \n",
    "#                     proportions_transported.append(0)\n",
    "#                     ci_lows.append(0)\n",
    "#                     ci_highs.append(0)\n",
    "\n",
    "#             prop_df = pd.DataFrame({\n",
    "#                 'category': categories, # Already strings due to .astype(str) in crosstab\n",
    "#                 'proportion_transported': proportions_transported,\n",
    "#                 'ci_low': ci_lows,\n",
    "#                 'ci_high': ci_highs\n",
    "#             })\n",
    "            \n",
    "#             ax_2_1.bar(prop_df['category'], prop_df['proportion_transported'], \n",
    "#                        yerr=[prop_df['proportion_transported'] - prop_df['ci_low'], prop_df['ci_high'] - prop_df['proportion_transported']],\n",
    "#                        capsize=5, color='mediumseagreen', alpha=0.7)\n",
    "#             ax_2_1.set_ylabel(f'Proportion {y_named.name}')\n",
    "#             ax_2_1.tick_params(axis='x', rotation=45) # Corrected: Apply rotation\n",
    "#             plt.setp(ax_2_1.get_xticklabels(), ha='right', rotation_mode='anchor') # Corrected: Ensure alignment\n",
    "#             ax_2_1.axhline(current_feature_data['Transported_numeric'].mean(), color='grey', linestyle='--', label='Overall Mean')\n",
    "#             if not prop_df.empty: # Only add legend if there's data to plot\n",
    "#                 ax_2_1.legend(loc='best')\n",
    "\n",
    "#             interpretation_text += \"Error bars on plot show 95% CIs for proportion transported. \"\n",
    "#             interpretation_text += \"If CIs for different categories don't overlap much, \"\n",
    "#             interpretation_text += \"it suggests a significant difference in transport rates.\\n\"\n",
    "\n",
    "#     else: # Numerical feature\n",
    "#         group0 = current_feature_data[current_feature_data['Transported_numeric'] == 0][feature_col].dropna() # Ensure NaNs are out for tests\n",
    "#         group1 = current_feature_data[current_feature_data['Transported_numeric'] == 1][feature_col].dropna() # Ensure NaNs are out for tests\n",
    "\n",
    "#         ax_2_1.set_title(f'{feature_col} Distribution by Target')\n",
    "#         # For boxplot, ensure data passed has NaNs handled if seaborn version is older\n",
    "#         sns.boxplot(x='Transported_numeric', y=feature_col, data=current_feature_data.dropna(subset=[feature_col]), \n",
    "#                     ax=ax_2_1, palette=palette, hue='Transported_numeric', legend=False)\n",
    "#         ax_2_1.set_xticklabels([legend_labels[0], legend_labels[1]])\n",
    "#         ax_2_1.set_xlabel(y_named.name)\n",
    "\n",
    "#         stats_summary_text += \"Normality (Shapiro-Wilk):\\n\"\n",
    "#         norm_p0, norm_p1 = -1.0, -1.0 # Initialize as float\n",
    "#         if len(group0) >=3 : \n",
    "#             shapiro_stat0, norm_p0 = stats.shapiro(group0)\n",
    "#             stats_summary_text += f\"  Group 0 (Not Transported): p={norm_p0:.3g}\\n\"\n",
    "#         else: stats_summary_text += \"  Group 0: Too few samples for normality test.\\n\"\n",
    "#         if len(group1) >=3 :\n",
    "#             shapiro_stat1, norm_p1 = stats.shapiro(group1)\n",
    "#             stats_summary_text += f\"  Group 1 (Transported): p={norm_p1:.3g}\\n\"\n",
    "#         else: stats_summary_text += \"  Group 1: Too few samples for normality test.\\n\"\n",
    "        \n",
    "#         # Default to Mann-Whitney U if any group has < 3 samples for normality, or if normality fails\n",
    "#         use_ttest = (norm_p0 > 0.05 or len(group0) < 3) and \\\n",
    "#                     (norm_p1 > 0.05 or len(group1) < 3)\n",
    "\n",
    "#         if use_ttest and len(group0)>1 and len(group1)>1:\n",
    "#             levene_stat, levene_p = stats.levene(group0, group1)\n",
    "#             stats_summary_text += f\"Homogeneity of Variances (Levene's test): p={levene_p:.3g}\\n\"\n",
    "#             equal_var = levene_p > 0.05\n",
    "\n",
    "#             t_stat, p_ttest = stats.ttest_ind(group0, group1, equal_var=equal_var) # nan_policy='omit' is default in newer scipy\n",
    "#             stats_summary_text += f\"Independent T-test (equal_var={equal_var}):\\n\"\n",
    "#             stats_summary_text += f\"  T-statistic: {t_stat:.2f}, P-value: {p_ttest:.3g}\\n\"\n",
    "#             interpretation_text += f\"P-value ({p_ttest:.3g}) from t-test: \"\n",
    "#             interpretation_text += \"Suggests \" + (\"a significant\" if p_ttest < 0.05 else \"no significant\") + \\\n",
    "#                                    f\" difference in mean {feature_col} between groups.\\n\"\n",
    "#             ax_2_1.text(0.5, 0.95, f\"T-test p-value: {p_ttest:.3g}\", ha='center', va='top', transform=ax_2_1.transAxes, fontsize=9, color='red', bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "#         elif len(group0)>0 and len(group1)>0: \n",
    "#             try:\n",
    "#                 u_stat, p_mannwhitney = stats.mannwhitneyu(group0, group1, alternative='two-sided') # nan_policy='omit' is default\n",
    "#                 stats_summary_text += f\"Mann-Whitney U Test:\\n\"\n",
    "#                 stats_summary_text += f\"  U-statistic: {u_stat:.0f}, P-value: {p_mannwhitney:.3g}\\n\"\n",
    "#                 interpretation_text += f\"P-value ({p_mannwhitney:.3g}) from Mann-Whitney U test: \"\n",
    "#                 interpretation_text += \"Suggests \" + (\"a significant\" if p_mannwhitney < 0.05 else \"no significant\") + \\\n",
    "#                                    f\" difference in distributions of {feature_col} between groups.\\n\"\n",
    "#                 ax_2_1.text(0.5, 0.95, f\"Mann-Whitney p: {p_mannwhitney:.3g}\", ha='center', va='top', transform=ax_2_1.transAxes, fontsize=9, color='red', bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "#             except ValueError as e_mw: \n",
    "#                  stats_summary_text += f\"Mann-Whitney U Test: Error - {e_mw}\\n\"\n",
    "#                  interpretation_text += \"Mann-Whitney U test could not be performed (e.g., identical data in groups).\\n\"\n",
    "#         else:\n",
    "#             stats_summary_text += \"Not enough data in one or both groups for numerical tests.\\n\"\n",
    "#             interpretation_text += \"Not enough data to compare groups statistically.\\n\"\n",
    "\n",
    "#     interpretation_text += \"\\nBayesian Perspective:\\nA Bayesian approach could provide posterior distributions for parameters \"\n",
    "#     interpretation_text += \"(e.g., difference in means/proportions). This offers a richer view of uncertainty \"\n",
    "#     interpretation_text += \"and allows direct probability statements about the effect size, rather than just a p-value.\"\n",
    "\n",
    "#     ax_2_2.text(0.01, 0.98, stats_summary_text, fontsize=8, va='top', ha='left', wrap=True, family='monospace') # Reduced font size\n",
    "#     ax_2_2.set_title(\"Statistical Test Details\", fontsize=10)\n",
    "    \n",
    "#     ax_2_3.text(0.01, 0.98, interpretation_text, fontsize=8, va='top', ha='left', wrap=True) # Reduced font size\n",
    "#     ax_2_3.set_title(\"Interpretation & Bayesian Note\", fontsize=10)\n",
    "\n",
    "#     fig.subplots_adjust(hspace=0.7, wspace=0.4, top=0.93, bottom=0.12, left=0.06, right=0.97) # Adjusted spacing\n",
    "#     plt.show()\n",
    "\n",
    "# print(\"--- Finished generating all plots. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Reloaded red_wine_quality.etl_chain, utils.etl, utils.eda, utils.eval, utils.submission\n",
      "Starting ETL process for dataset: space-titanic with target: Transported\n",
      "Attempting to load from: /home/jovyan/data/space-titanic/raw/train.csv\n",
      "Dataset loaded successfully from /home/jovyan/data/space-titanic/raw/train.csv. Shape: (8693, 14)\n",
      "Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',\n",
      "       'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
      "       'Name'],\n",
      "      dtype='object')\n",
      "--- Fitting pipeline on Training Data ---\n",
      "Fitting step in chain: RawTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n",
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting step in chain: KNNImputerComponent\n",
      "Fitting step in chain: NameWordFeatures\n",
      "Fitting step in chain: FeatureGenerators\n",
      "Fitting step in chain: ServiceFeaturesComponent\n",
      "\n",
      "--- Transforming Training Data (Streaming) ---\n",
      "STREAM_MSG (Train): [ETLChain] Status: starting, Msg: Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: starting, Msg: Chain transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Starting sub-step: RawTransformer (1/5), Progress: N/A\n",
      "STREAM_MSG (Train): [RawTransformer] Status: starting, Msg: [Chain -> RawTransformer] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Starting raw transformations., Progress: N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n",
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAM_MSG (Train): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Raw transformations applied., Progress: 1.0\n",
      "STREAM_MSG (Train): [RawTransformer] Status: completed, Msg: [Chain -> RawTransformer] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Completed sub-step: RawTransformer. Shape after: (6954, 16), Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Starting sub-step: KNNImputerComponent (2/5), Progress: N/A\n",
      "STREAM_MSG (Train): [KNNImputerComponent] Status: starting, Msg: [Chain -> KNNImputerComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] Starting KNN imputation., Progress: N/A\n",
      "STREAM_MSG (Train): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] KNN imputation applied if necessary., Progress: 1.0\n",
      "STREAM_MSG (Train): [KNNImputerComponent] Status: completed, Msg: [Chain -> KNNImputerComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Completed sub-step: KNNImputerComponent. Shape after: (6954, 16), Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Starting sub-step: NameWordFeatures (3/5), Progress: N/A\n",
      "STREAM_MSG (Train): [NameWordFeatures] Status: starting, Msg: [Chain -> NameWordFeatures] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Applying name word features., Progress: N/A\n",
      "STREAM_MSG (Train): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Name word features applied., Progress: 1.0\n",
      "STREAM_MSG (Train): [NameWordFeatures] Status: completed, Msg: [Chain -> NameWordFeatures] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Completed sub-step: NameWordFeatures. Shape after: (6954, 19), Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Starting sub-step: FeatureGenerators (4/5), Progress: N/A\n",
      "STREAM_MSG (Train): [FeatureGenerators] Status: starting, Msg: [Chain -> FeatureGenerators] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Starting feature generation., Progress: N/A\n",
      "STREAM_MSG (Train): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Feature generation applied., Progress: 1.0\n",
      "STREAM_MSG (Train): [FeatureGenerators] Status: completed, Msg: [Chain -> FeatureGenerators] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Completed sub-step: FeatureGenerators. Shape after: (6954, 37), Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Starting sub-step: ServiceFeaturesComponent (5/5), Progress: N/A\n",
      "STREAM_MSG (Train): [ServiceFeaturesComponent] Status: starting, Msg: [Chain -> ServiceFeaturesComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Starting service feature engineering., Progress: N/A\n",
      "STREAM_MSG (Train): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Service features engineered and original columns dropped., Progress: 1.0\n",
      "STREAM_MSG (Train): [ServiceFeaturesComponent] Status: completed, Msg: [Chain -> ServiceFeaturesComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Completed sub-step: ServiceFeaturesComponent. Shape after: (6954, 52), Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: completed, Msg: Chain transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: completed, Msg: Transformation complete., Progress: N/A\n",
      "\n",
      "--- Transforming Validation Data (Streaming) ---\n",
      "STREAM_MSG (Val): [ETLChain] Status: starting, Msg: Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: starting, Msg: Chain transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Starting sub-step: RawTransformer (1/5), Progress: N/A\n",
      "STREAM_MSG (Val): [RawTransformer] Status: starting, Msg: [Chain -> RawTransformer] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Starting raw transformations., Progress: N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n",
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAM_MSG (Val): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Raw transformations applied., Progress: 1.0\n",
      "STREAM_MSG (Val): [RawTransformer] Status: completed, Msg: [Chain -> RawTransformer] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Completed sub-step: RawTransformer. Shape after: (1739, 16), Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Starting sub-step: KNNImputerComponent (2/5), Progress: N/A\n",
      "STREAM_MSG (Val): [KNNImputerComponent] Status: starting, Msg: [Chain -> KNNImputerComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] Starting KNN imputation., Progress: N/A\n",
      "STREAM_MSG (Val): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] KNN imputation applied if necessary., Progress: 1.0\n",
      "STREAM_MSG (Val): [KNNImputerComponent] Status: completed, Msg: [Chain -> KNNImputerComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Completed sub-step: KNNImputerComponent. Shape after: (1739, 16), Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Starting sub-step: NameWordFeatures (3/5), Progress: N/A\n",
      "STREAM_MSG (Val): [NameWordFeatures] Status: starting, Msg: [Chain -> NameWordFeatures] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Applying name word features., Progress: N/A\n",
      "STREAM_MSG (Val): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Name word features applied., Progress: 1.0\n",
      "STREAM_MSG (Val): [NameWordFeatures] Status: completed, Msg: [Chain -> NameWordFeatures] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Completed sub-step: NameWordFeatures. Shape after: (1739, 19), Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Starting sub-step: FeatureGenerators (4/5), Progress: N/A\n",
      "STREAM_MSG (Val): [FeatureGenerators] Status: starting, Msg: [Chain -> FeatureGenerators] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Starting feature generation., Progress: N/A\n",
      "STREAM_MSG (Val): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Feature generation applied., Progress: 1.0\n",
      "STREAM_MSG (Val): [FeatureGenerators] Status: completed, Msg: [Chain -> FeatureGenerators] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Completed sub-step: FeatureGenerators. Shape after: (1739, 37), Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Starting sub-step: ServiceFeaturesComponent (5/5), Progress: N/A\n",
      "STREAM_MSG (Val): [ServiceFeaturesComponent] Status: starting, Msg: [Chain -> ServiceFeaturesComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Starting service feature engineering., Progress: N/A\n",
      "STREAM_MSG (Val): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Service features engineered and original columns dropped., Progress: 1.0\n",
      "STREAM_MSG (Val): [ServiceFeaturesComponent] Status: completed, Msg: [Chain -> ServiceFeaturesComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Completed sub-step: ServiceFeaturesComponent. Shape after: (1739, 52), Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: completed, Msg: Chain transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: completed, Msg: Transformation complete., Progress: N/A\n",
      "\n",
      "--- Loading and Transforming Test Data ---\n",
      "Attempting to load from: /home/jovyan/data/space-titanic/test/test.csv\n",
      "Dataset loaded successfully from /home/jovyan/data/space-titanic/test/test.csv. Shape: (4277, 13)\n",
      "STREAM_MSG (Test): [ETLChain] Status: starting, Msg: Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: starting, Msg: Chain transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Starting sub-step: RawTransformer (1/5), Progress: N/A\n",
      "STREAM_MSG (Test): [RawTransformer] Status: starting, Msg: [Chain -> RawTransformer] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Starting raw transformations., Progress: N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n",
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAM_MSG (Test): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Raw transformations applied., Progress: 1.0\n",
      "STREAM_MSG (Test): [RawTransformer] Status: completed, Msg: [Chain -> RawTransformer] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Completed sub-step: RawTransformer. Shape after: (4277, 16), Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Starting sub-step: KNNImputerComponent (2/5), Progress: N/A\n",
      "STREAM_MSG (Test): [KNNImputerComponent] Status: starting, Msg: [Chain -> KNNImputerComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] Starting KNN imputation., Progress: N/A\n",
      "STREAM_MSG (Test): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] KNN imputation applied if necessary., Progress: 1.0\n",
      "STREAM_MSG (Test): [KNNImputerComponent] Status: completed, Msg: [Chain -> KNNImputerComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Completed sub-step: KNNImputerComponent. Shape after: (4277, 16), Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Starting sub-step: NameWordFeatures (3/5), Progress: N/A\n",
      "STREAM_MSG (Test): [NameWordFeatures] Status: starting, Msg: [Chain -> NameWordFeatures] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Applying name word features., Progress: N/A\n",
      "STREAM_MSG (Test): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Name word features applied., Progress: 1.0\n",
      "STREAM_MSG (Test): [NameWordFeatures] Status: completed, Msg: [Chain -> NameWordFeatures] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Completed sub-step: NameWordFeatures. Shape after: (4277, 19), Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Starting sub-step: FeatureGenerators (4/5), Progress: N/A\n",
      "STREAM_MSG (Test): [FeatureGenerators] Status: starting, Msg: [Chain -> FeatureGenerators] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Starting feature generation., Progress: N/A\n",
      "STREAM_MSG (Test): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Feature generation applied., Progress: 1.0\n",
      "STREAM_MSG (Test): [FeatureGenerators] Status: completed, Msg: [Chain -> FeatureGenerators] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Completed sub-step: FeatureGenerators. Shape after: (4277, 37), Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Starting sub-step: ServiceFeaturesComponent (5/5), Progress: N/A\n",
      "STREAM_MSG (Test): [ServiceFeaturesComponent] Status: starting, Msg: [Chain -> ServiceFeaturesComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Starting service feature engineering., Progress: N/A\n",
      "STREAM_MSG (Test): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Service features engineered and original columns dropped., Progress: 1.0\n",
      "STREAM_MSG (Test): [ServiceFeaturesComponent] Status: completed, Msg: [Chain -> ServiceFeaturesComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Completed sub-step: ServiceFeaturesComponent. Shape after: (4277, 52), Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: completed, Msg: Chain transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: completed, Msg: Transformation complete., Progress: N/A\n",
      "\n",
      "--- ETL Process Complete ---\n",
      "X_train shape: (6954, 52), X_val shape: (1739, 52)\n",
      "X_test shape: (4277, 52)\n",
      "X_test shape: (4277, 52)\n",
      "âœ… X schema valid.\n",
      "âœ… X schema valid.\n",
      "âœ… y schema valid.\n",
      "âœ… y schema valid.\n",
      "(6954, 13)\n",
      "(6954, 52)\n",
      "(1739, 52)\n",
      "âœ… RandomForest Accuracy (original): 0.7729\n",
      "\n",
      "--- XGBoost ---\n",
      "âœ… XGBoost Accuracy: 0.7786\n",
      "\n",
      "ðŸ“Š Top 10 Features by RF Importance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>rf_importance</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>pearson_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>total_spent</td>\n",
       "      <td>0.061795</td>\n",
       "      <td>0.132651</td>\n",
       "      <td>-0.203060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>max_spend_category</td>\n",
       "      <td>0.053743</td>\n",
       "      <td>0.084929</td>\n",
       "      <td>-0.366842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>kmeans_dist_4</td>\n",
       "      <td>0.053212</td>\n",
       "      <td>0.102992</td>\n",
       "      <td>-0.402393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>gmm_cluster_prob_4</td>\n",
       "      <td>0.050810</td>\n",
       "      <td>0.135684</td>\n",
       "      <td>0.312152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CryoSleep</td>\n",
       "      <td>0.047056</td>\n",
       "      <td>0.118921</td>\n",
       "      <td>0.462554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ica_5</td>\n",
       "      <td>0.044654</td>\n",
       "      <td>0.102559</td>\n",
       "      <td>-0.026516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ica_2</td>\n",
       "      <td>0.042330</td>\n",
       "      <td>0.075968</td>\n",
       "      <td>0.340686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kmeans_dist_3</td>\n",
       "      <td>0.035811</td>\n",
       "      <td>0.045384</td>\n",
       "      <td>0.277533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ica_1</td>\n",
       "      <td>0.034857</td>\n",
       "      <td>0.059122</td>\n",
       "      <td>0.208440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ica_7</td>\n",
       "      <td>0.031901</td>\n",
       "      <td>0.065266</td>\n",
       "      <td>0.226743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature  rf_importance  mutual_info  pearson_corr\n",
       "5          total_spent       0.061795     0.132651     -0.203060\n",
       "6   max_spend_category       0.053743     0.084929     -0.366842\n",
       "26       kmeans_dist_4       0.053212     0.102992     -0.402393\n",
       "31  gmm_cluster_prob_4       0.050810     0.135684      0.312152\n",
       "1            CryoSleep       0.047056     0.118921      0.462554\n",
       "18               ica_5       0.044654     0.102559     -0.026516\n",
       "15               ica_2       0.042330     0.075968      0.340686\n",
       "25       kmeans_dist_3       0.035811     0.045384      0.277533\n",
       "14               ica_1       0.034857     0.059122      0.208440\n",
       "20               ica_7       0.031901     0.065266      0.226743"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¹ Least Informative Features (below threshold):\n",
      "['ica_5', 'ica_2', 'kmeans_dist_3', 'ica_1', 'ica_7', 'ica_4', 'kmeans_dist_2', 'kmeans_dist_1', 'ica_6', 'ica_3', 'cabin_num', 'kmeans_dist_0', 'gmm_cluster_prob_3', 'gmm_cluster_prob_0', 'gmm_cluster_prob_2', 'Age', 'RoomService_tp25', 'RoomService_tp75', 'name_corr_neg_p_value', 'has_used_RoomService', 'gmm_cluster_prob_1', 'RoomService_tp50', 'name_corr_neg_prob', 'has_used_Spa', 'Spa_tp50', 'deck', 'Spa_tp75', 'Spa_tp25', 'VRDeck_tp25', 'HomePlanet', 'kmeans_cluster', 'has_used_VRDeck', 'name_corr_pos_prob', 'name_corr_pos_p_value', 'side', 'VRDeck_tp50', 'ShoppingMall_tp50', 'has_used_ShoppingMall', 'VRDeck_tp75', 'FoodCourt_tp75', 'ShoppingMall_tp25', 'Destination', 'ShoppingMall_tp75', 'FoodCourt_tp25', 'has_used_FoodCourt', 'FoodCourt_tp50', 'VIP']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>rf_importance</th>\n",
       "      <th>mutual_info</th>\n",
       "      <th>pearson_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>has_used_ShoppingMall</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.033426</td>\n",
       "      <td>-0.269141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>VRDeck_tp75</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>0.048045</td>\n",
       "      <td>-0.290732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>FoodCourt_tp75</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.012627</td>\n",
       "      <td>-0.120046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ShoppingMall_tp25</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.028103</td>\n",
       "      <td>-0.269141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Destination</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>-0.094455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ShoppingMall_tp75</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>-0.191316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>FoodCourt_tp25</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.028891</td>\n",
       "      <td>-0.230309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>has_used_FoodCourt</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.021268</td>\n",
       "      <td>-0.230309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>FoodCourt_tp50</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.033053</td>\n",
       "      <td>-0.230309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VIP</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.040546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature  rf_importance  mutual_info  pearson_corr\n",
       "40  has_used_ShoppingMall       0.003929     0.033426     -0.269141\n",
       "51            VRDeck_tp75       0.003914     0.048045     -0.290732\n",
       "39         FoodCourt_tp75       0.003681     0.012627     -0.120046\n",
       "41      ShoppingMall_tp25       0.003445     0.028103     -0.269141\n",
       "2             Destination       0.003391     0.010600     -0.094455\n",
       "43      ShoppingMall_tp75       0.003195     0.007285     -0.191316\n",
       "37         FoodCourt_tp25       0.002359     0.028891     -0.230309\n",
       "36     has_used_FoodCourt       0.002307     0.021268     -0.230309\n",
       "38         FoodCourt_tp50       0.001974     0.033053     -0.230309\n",
       "4                     VIP       0.000706     0.000000     -0.040546"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” RandomForest Accuracy after dropping: 0.7510\n",
      "ðŸ”´ Degraded by -0.0219\n"
     ]
    }
   ],
   "source": [
    "reload_utils()\n",
    "import pandas as pd\n",
    "from etl_chain import run_custom_etl_streaming\n",
    "# --- 1. Run the ETL pipeline from etl_chain.py ---\n",
    "print(f\"Starting ETL process for dataset: {local_dataset_name} with target: {target_column}\")\n",
    "try:\n",
    "    etl_result = run_custom_etl_streaming(dataset_name=local_dataset_name, target_column=target_column)\n",
    "    X_train_unprocessed = etl_result.get(\"X_train_unprocessed\")\n",
    "    X_train = etl_result.get(\"X_train\")\n",
    "    y_train = etl_result.get(\"y_train\")\n",
    "    X_val = etl_result.get(\"X_val\")\n",
    "    y_val = etl_result.get(\"y_val\")    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the ETL process: {e}\")\n",
    "    raise\n",
    "\n",
    "from utils.etl import validate_X_schema, validate_y_schema\n",
    "validate_X_schema(X_train)\n",
    "validate_X_schema(X_val)\n",
    "validate_y_schema(y_train)\n",
    "validate_y_schema(y_val)\n",
    "# X_train.isna().sum()\n",
    "print(X_train_unprocessed.shape)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "from utils.eval import evaluate_feature_feedback\n",
    "feedback = evaluate_feature_feedback(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    auto_tune_threshold=False,  # ðŸ” enable random search\n",
    "    drop_threshold=0.045,\n",
    "    n_trials=30,\n",
    "    auto_drop=True,\n",
    ")\n",
    "\n",
    "# Use the optimized result\n",
    "# X_train = feedback[\"X_train_clean\"]\n",
    "# X_val = feedback[\"X_val_clean\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import get_sklearn_model, get_nn_model\n",
    "from utils.train import train_sklearn_model, train_nn_model, log_model_artifact, log_final_metrics, evaluate_model, \\\n",
    "                        conditionally_encode_labels\n",
    "import mlflow\n",
    "os.environ[\"WANDB_BASE_URL\"] = \"http://wandb:8080\"\n",
    "os.environ[\"WANDB_DEBUG\"] = \"true\"\n",
    "os.environ[\"WANDB_DEBUG_LOG_PATH\"] = \"/tmp/wandb_debug.log\"\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define model sweep\n",
    "print(\"defining sweep\")\n",
    "model_configs = {\n",
    "    # \"decision_tree\": [\n",
    "    #     {\"criterion\": c, \"max_depth\": d, \"min_samples_split\": m, \"min_samples_leaf\": l}\n",
    "    #     for c in [\"gini\", \"entropy\"]\n",
    "    #     for d in [None, 5, 10]\n",
    "    #     for m in [2, 5]\n",
    "    #     for l in [1, 2]\n",
    "    # ],\n",
    "\n",
    "    \"random_forest\": [\n",
    "        {\"n_estimators\": n, \"max_depth\": d, \"max_features\": f, \"min_samples_split\": m}\n",
    "        for n in [50, 200, 350]\n",
    "        for d in [None, 10,15]\n",
    "        for f in [\"sqrt\", \"log2\"]\n",
    "        for m in [2, 5]\n",
    "    ],\n",
    "\n",
    "    # \"logistic_regression\": [\n",
    "    #     {\"penalty\": p, \"C\": c, \"solver\": \"liblinear\", \"max_iter\": 200}\n",
    "    #     for p in [\"l1\", \"l2\"]\n",
    "    #     for c in [0.01, 0.1, 1.0]\n",
    "    # ],\n",
    "\n",
    "    \"xgboost\": [\n",
    "        {\"n_estimators\": n, \"max_depth\": d, \"learning_rate\": lr, \"subsample\": s, \"colsample_bytree\": cbt}\n",
    "        for n in [200]\n",
    "        for d in [5, 7]\n",
    "        for lr in [0.005, 0.1]\n",
    "        for s in [0.8, 1.0]\n",
    "        for cbt in [0.8, 1.0]\n",
    "    ],\n",
    "\n",
    "    \"lightgbm\": [\n",
    "        {\"n_estimators\": n, \"max_depth\": d, \"learning_rate\": lr, \"num_leaves\": nl, \"min_child_samples\": mcs}\n",
    "        for n in [100, 200]\n",
    "        for d in [-1, 10]\n",
    "        for lr in [0.01, 0.1]\n",
    "        for nl in [31, 50]\n",
    "        for mcs in [10, 20]\n",
    "    ],\n",
    "    # \"svm\": [\n",
    "    #     {\"C\": c, \"kernel\": k, \"gamma\": g}\n",
    "    #     for c in [0.1, 1.0, 10.0]\n",
    "    #     for k in [\"linear\", \"rbf\", \"poly\"]\n",
    "    #     for g in [\"scale\", \"auto\"]\n",
    "    # ],\n",
    "\n",
    "    # \"naive_bayes\": [\n",
    "    #     {\"var_smoothing\": vs}\n",
    "    #     for vs in [1e-9, 1e-8, 1e-7]\n",
    "    # ],\n",
    "\n",
    "    # \"knn\": [\n",
    "    #     {\"n_neighbors\": k, \"weights\": w, \"metric\": m}\n",
    "    #     for k in [ 5, 7]\n",
    "    #     for w in [\"uniform\", \"distance\"]\n",
    "    #     for m in [\"euclidean\", \"manhattan\"]\n",
    "    # ],\n",
    "    # \"neural_net\": [\n",
    "    #     {\n",
    "    #         \"model_type\": mt,\n",
    "    #         \"hidden\": h,\n",
    "    #         \"dropout\": d,\n",
    "    #         \"activation\": act,\n",
    "    #         \"batch_norm\": bn,\n",
    "    #         \"lr\": lr,\n",
    "    #         \"num_layers\": nl\n",
    "    #     }\n",
    "    #     for mt in [\"mlp\", \"lstm\", \"cnn\"]\n",
    "    #     for h in [32, 64]\n",
    "    #     for d in [0.0, 0.3]\n",
    "    #     for act in [\"relu\", \"tanh\"]\n",
    "    #     for bn in [False, True]\n",
    "    #     for lr in [.005, 0.001, .05, 0.01, .05]\n",
    "    #     for nl in [1, 2, 3,4]\n",
    "    # ],\n",
    "}\n",
    "\n",
    "mlflow.set_experiment(f\"kaggle_{local_dataset_name}\")\n",
    "print(\"starting experiments\")\n",
    "# Loop over each model and its hyperparam\n",
    "for model_name, config_list in model_configs.items():\n",
    "    for params in config_list:\n",
    "        # Terminate any already running experiments (MLflow and W&B)\n",
    "        if mlflow.active_run() is not None:\n",
    "            mlflow.end_run()\n",
    "        wandb.finish()\n",
    "        # === Init Experiment | MLflow and W&B ===\n",
    "        run_name = f\"{etl_result['etl_version']}_{model_name}_{params}\"\n",
    "        run = mlflow.start_run(run_name=run_name)\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"task_type\", task_type)\n",
    "        mlflow.log_param(\"perf_eval_metric\", perf_eval_metric)\n",
    "        mlflow.log_param(\"etl_version\", etl_result['etl_version'])\n",
    "        mlflow.log_param(\"etl_description\", etl_result['etl_description'])\n",
    "        print(\"Started MLflow run:\", run.info.run_id)\n",
    "        wandb.init(\n",
    "            project=f\"kaggle_{local_dataset_name}\",\n",
    "            name=run_name,\n",
    "            config={\n",
    "                **params,\n",
    "                \"model_name\": model_name,\n",
    "                \"task_type\": task_type,\n",
    "                \"perf_eval_metric\": perf_eval_metric,\n",
    "                \"etl_version\": etl_result['etl_version'],\n",
    "                \"etl_description\": etl_result['etl_description']\n",
    "            }\n",
    "        )\n",
    "        # === Train ===\n",
    "        if model_name == \"neural_net\":\n",
    "            output_dim = y_train.nunique() if task_type == \"multiclass_classification\" else 1\n",
    "            model, lr = get_nn_model(X_train.shape[1], output_dim=output_dim, **params)\n",
    "            y_train_nn, y_val_nn, label_encoder, label_encoder_applied = conditionally_encode_labels(y_train, y_val)\n",
    "            model, y_pred = train_nn_model(\n",
    "                model, X_train, y_train_nn, X_val, y_val_nn,\n",
    "                epochs=20,\n",
    "                lr = lr,\n",
    "                task_type=task_type,\n",
    "                eval_metric_name=eval_metric_name,\n",
    "                eval_metric_fn=eval_metric_fn,\n",
    "            )\n",
    "            if label_encoder_applied:\n",
    "                y_pred = label_encoder.inverse_transform(y_pred)\n",
    "            print(\"âœ… Finished training neural net\")\n",
    "\n",
    "        else:\n",
    "            model = get_sklearn_model(model_name, **params)\n",
    "            trained_model, y_pred = train_sklearn_model(\n",
    "                model, X_train, y_train, X_val, y_val,\n",
    "                task_type=task_type,\n",
    "            )\n",
    "            print(\"âœ… Finished training sklearn model\")\n",
    "\n",
    "        # === Evaluate & log ===\n",
    "        eval_metrics = evaluate_model(y_val, y_pred, task_type)\n",
    "        log_final_metrics(eval_metrics)\n",
    "        # commented out to reduced write2dis\n",
    "        # log_model_artifact(trained_model, model_name, framework=\"torch\" if model_name == \"neural_net\" else \"sklearn\")\n",
    "\n",
    "        # === End Experiment | MLflow and W&B ===\n",
    "        mlflow.end_run()\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Reloaded red_wine_quality.etl_chain, utils.etl, utils.eda, utils.eval, utils.submission\n",
      "Starting ETL process for dataset: space-titanic with target: Transported\n",
      "Attempting to load from: /home/jovyan/data/space-titanic/raw/train.csv\n",
      "Dataset loaded successfully from /home/jovyan/data/space-titanic/raw/train.csv. Shape: (8693, 14)\n",
      "Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',\n",
      "       'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
      "       'Name'],\n",
      "      dtype='object')\n",
      "--- Fitting pipeline on Training Data ---\n",
      "Fitting step in chain: RawTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n",
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting step in chain: KNNImputerComponent\n",
      "Fitting step in chain: NameWordFeatures\n",
      "Warning: Name column 'Name' not found in X_train for NameWordFeatures fitting. State will be empty for word_stats.\n",
      "Fitting step in chain: FeatureGenerators\n",
      "Fitting step in chain: ServiceFeaturesComponent\n",
      "\n",
      "--- Transforming Training Data (Streaming) ---\n",
      "STREAM_MSG (Train): [ETLChain] Status: starting, Msg: Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: starting, Msg: Chain transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Starting sub-step: RawTransformer (1/5), Progress: N/A\n",
      "STREAM_MSG (Train): [RawTransformer] Status: starting, Msg: [Chain -> RawTransformer] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Starting raw transformations., Progress: N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n",
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAM_MSG (Train): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Raw transformations applied., Progress: 1.0\n",
      "STREAM_MSG (Train): [RawTransformer] Status: completed, Msg: [Chain -> RawTransformer] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Completed sub-step: RawTransformer. Shape after: (8606, 15), Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Starting sub-step: KNNImputerComponent (2/5), Progress: N/A\n",
      "STREAM_MSG (Train): [KNNImputerComponent] Status: starting, Msg: [Chain -> KNNImputerComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] Starting KNN imputation., Progress: N/A\n",
      "STREAM_MSG (Train): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] KNN imputation applied if necessary., Progress: 1.0\n",
      "STREAM_MSG (Train): [KNNImputerComponent] Status: completed, Msg: [Chain -> KNNImputerComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Completed sub-step: KNNImputerComponent. Shape after: (8606, 15), Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Starting sub-step: NameWordFeatures (3/5), Progress: N/A\n",
      "STREAM_MSG (Train): [NameWordFeatures] Status: starting, Msg: [Chain -> NameWordFeatures] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Applying name word features., Progress: N/A\n",
      "STREAM_MSG (Train): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Name word features applied., Progress: 1.0\n",
      "STREAM_MSG (Train): [NameWordFeatures] Status: completed, Msg: [Chain -> NameWordFeatures] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Completed sub-step: NameWordFeatures. Shape after: (8606, 15), Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Starting sub-step: FeatureGenerators (4/5), Progress: N/A\n",
      "STREAM_MSG (Train): [FeatureGenerators] Status: starting, Msg: [Chain -> FeatureGenerators] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Starting feature generation., Progress: N/A\n",
      "STREAM_MSG (Train): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Feature generation applied., Progress: 1.0\n",
      "STREAM_MSG (Train): [FeatureGenerators] Status: completed, Msg: [Chain -> FeatureGenerators] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Completed sub-step: FeatureGenerators. Shape after: (8606, 33), Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Starting sub-step: ServiceFeaturesComponent (5/5), Progress: N/A\n",
      "STREAM_MSG (Train): [ServiceFeaturesComponent] Status: starting, Msg: [Chain -> ServiceFeaturesComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Train): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Starting service feature engineering., Progress: N/A\n",
      "STREAM_MSG (Train): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Service features engineered and original columns dropped., Progress: 1.0\n",
      "STREAM_MSG (Train): [ServiceFeaturesComponent] Status: completed, Msg: [Chain -> ServiceFeaturesComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: in_progress, Msg: Completed sub-step: ServiceFeaturesComponent. Shape after: (8606, 48), Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: completed, Msg: Chain transformation complete., Progress: N/A\n",
      "STREAM_MSG (Train): [ETLChain] Status: completed, Msg: Transformation complete., Progress: N/A\n",
      "\n",
      "--- Transforming Validation Data (Streaming) ---\n",
      "STREAM_MSG (Val): [ETLChain] Status: starting, Msg: Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: starting, Msg: Chain transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Starting sub-step: RawTransformer (1/5), Progress: N/A\n",
      "STREAM_MSG (Val): [RawTransformer] Status: starting, Msg: [Chain -> RawTransformer] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Starting raw transformations., Progress: N/A\n",
      "STREAM_MSG (Val): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Raw transformations applied., Progress: 1.0\n",
      "STREAM_MSG (Val): [RawTransformer] Status: completed, Msg: [Chain -> RawTransformer] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Completed sub-step: RawTransformer. Shape after: (87, 15), Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Starting sub-step: KNNImputerComponent (2/5), Progress: N/A\n",
      "STREAM_MSG (Val): [KNNImputerComponent] Status: starting, Msg: [Chain -> KNNImputerComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] Starting KNN imputation., Progress: N/A\n",
      "STREAM_MSG (Val): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] KNN imputation applied if necessary., Progress: 1.0\n",
      "STREAM_MSG (Val): [KNNImputerComponent] Status: completed, Msg: [Chain -> KNNImputerComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Completed sub-step: KNNImputerComponent. Shape after: (87, 15), Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Starting sub-step: NameWordFeatures (3/5), Progress: N/A\n",
      "STREAM_MSG (Val): [NameWordFeatures] Status: starting, Msg: [Chain -> NameWordFeatures] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Applying name word features., Progress: N/A\n",
      "STREAM_MSG (Val): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Name word features applied., Progress: 1.0\n",
      "STREAM_MSG (Val): [NameWordFeatures] Status: completed, Msg: [Chain -> NameWordFeatures] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Completed sub-step: NameWordFeatures. Shape after: (87, 15), Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Starting sub-step: FeatureGenerators (4/5), Progress: N/A\n",
      "STREAM_MSG (Val): [FeatureGenerators] Status: starting, Msg: [Chain -> FeatureGenerators] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Starting feature generation., Progress: N/A\n",
      "STREAM_MSG (Val): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Feature generation applied., Progress: 1.0\n",
      "STREAM_MSG (Val): [FeatureGenerators] Status: completed, Msg: [Chain -> FeatureGenerators] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Completed sub-step: FeatureGenerators. Shape after: (87, 33), Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Starting sub-step: ServiceFeaturesComponent (5/5), Progress: N/A\n",
      "STREAM_MSG (Val): [ServiceFeaturesComponent] Status: starting, Msg: [Chain -> ServiceFeaturesComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Val): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Starting service feature engineering., Progress: N/A\n",
      "STREAM_MSG (Val): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Service features engineered and original columns dropped., Progress: 1.0\n",
      "STREAM_MSG (Val): [ServiceFeaturesComponent] Status: completed, Msg: [Chain -> ServiceFeaturesComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: in_progress, Msg: Completed sub-step: ServiceFeaturesComponent. Shape after: (87, 48), Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: completed, Msg: Chain transformation complete., Progress: N/A\n",
      "STREAM_MSG (Val): [ETLChain] Status: completed, Msg: Transformation complete., Progress: N/A\n",
      "\n",
      "--- Loading and Transforming Test Data ---\n",
      "Attempting to load from: /home/jovyan/data/space-titanic/test/test.csv\n",
      "Dataset loaded successfully from /home/jovyan/data/space-titanic/test/test.csv. Shape: (4277, 13)\n",
      "STREAM_MSG (Test): [ETLChain] Status: starting, Msg: Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: starting, Msg: Chain transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Starting sub-step: RawTransformer (1/5), Progress: N/A\n",
      "STREAM_MSG (Test): [RawTransformer] Status: starting, Msg: [Chain -> RawTransformer] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Starting raw transformations., Progress: N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n",
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n",
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n",
      "/home/jovyan/workspace/red_wine_quality/etl_chain.py:392: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_df[col] = X_df[col].fillna(False).astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAM_MSG (Test): [RawTransformer] Status: in_progress, Msg: [Chain -> RawTransformer] Raw transformations applied., Progress: 1.0\n",
      "STREAM_MSG (Test): [RawTransformer] Status: completed, Msg: [Chain -> RawTransformer] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Completed sub-step: RawTransformer. Shape after: (4277, 15), Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Starting sub-step: KNNImputerComponent (2/5), Progress: N/A\n",
      "STREAM_MSG (Test): [KNNImputerComponent] Status: starting, Msg: [Chain -> KNNImputerComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] Starting KNN imputation., Progress: N/A\n",
      "STREAM_MSG (Test): [KNNImputerComponent] Status: in_progress, Msg: [Chain -> KNNImputerComponent] KNN imputation applied if necessary., Progress: 1.0\n",
      "STREAM_MSG (Test): [KNNImputerComponent] Status: completed, Msg: [Chain -> KNNImputerComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Completed sub-step: KNNImputerComponent. Shape after: (4277, 15), Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Starting sub-step: NameWordFeatures (3/5), Progress: N/A\n",
      "STREAM_MSG (Test): [NameWordFeatures] Status: starting, Msg: [Chain -> NameWordFeatures] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Applying name word features., Progress: N/A\n",
      "STREAM_MSG (Test): [NameWordFeatures] Status: in_progress, Msg: [Chain -> NameWordFeatures] Name word features applied., Progress: 1.0\n",
      "STREAM_MSG (Test): [NameWordFeatures] Status: completed, Msg: [Chain -> NameWordFeatures] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Completed sub-step: NameWordFeatures. Shape after: (4277, 15), Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Starting sub-step: FeatureGenerators (4/5), Progress: N/A\n",
      "STREAM_MSG (Test): [FeatureGenerators] Status: starting, Msg: [Chain -> FeatureGenerators] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Starting feature generation., Progress: N/A\n",
      "STREAM_MSG (Test): [FeatureGenerators] Status: in_progress, Msg: [Chain -> FeatureGenerators] Feature generation applied., Progress: 1.0\n",
      "STREAM_MSG (Test): [FeatureGenerators] Status: completed, Msg: [Chain -> FeatureGenerators] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Completed sub-step: FeatureGenerators. Shape after: (4277, 33), Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Starting sub-step: ServiceFeaturesComponent (5/5), Progress: N/A\n",
      "STREAM_MSG (Test): [ServiceFeaturesComponent] Status: starting, Msg: [Chain -> ServiceFeaturesComponent] Transformation starting., Progress: N/A\n",
      "STREAM_MSG (Test): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Starting service feature engineering., Progress: N/A\n",
      "STREAM_MSG (Test): [ServiceFeaturesComponent] Status: in_progress, Msg: [Chain -> ServiceFeaturesComponent] Service features engineered and original columns dropped., Progress: 1.0\n",
      "STREAM_MSG (Test): [ServiceFeaturesComponent] Status: completed, Msg: [Chain -> ServiceFeaturesComponent] Transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: in_progress, Msg: Completed sub-step: ServiceFeaturesComponent. Shape after: (4277, 48), Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: completed, Msg: Chain transformation complete., Progress: N/A\n",
      "STREAM_MSG (Test): [ETLChain] Status: completed, Msg: Transformation complete., Progress: N/A\n",
      "\n",
      "--- ETL Process Complete ---\n",
      "X_train shape: (8606, 48), X_val shape: (87, 48)\n",
      "X_test shape: (4277, 48)\n",
      "X_test shape: (4277, 48)\n",
      "\n",
      "--- Training lightgbm model ---\n",
      "âœ… Labels already normalized, no encoding needed\n",
      "[LightGBM] [Info] Number of positive: 4334, number of negative: 4272\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4998\n",
      "[LightGBM] [Info] Number of data points in the train set: 8606, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503602 -> initscore=0.014409\n",
      "[LightGBM] [Info] Start training from score 0.014409\n",
      "âœ… Finished training lightgbm model.\n",
      "\n",
      "--- Making predictions on test data ---\n",
      "\n",
      "--- Creating submission file: submission.csv ---\n",
      "ðŸŽ‰ submission.csv generated successfully!\n",
      "  PassengerId  Transported\n",
      "0     0013_01         True\n",
      "1     0018_01        False\n",
      "2     0019_01         True\n",
      "3     0021_01         True\n",
      "4     0023_01        False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reload_utils()\n",
    "from etl_chain import run_custom_etl_streaming\n",
    "from utils.models import get_sklearn_model \n",
    "from utils.train import train_sklearn_model \n",
    "from utils.submission import create_submission_file \n",
    "\n",
    "# --- 1. Run the ETL pipeline from etl_chain.py ---\n",
    "print(f\"Starting ETL process for dataset: {local_dataset_name} with target: {target_column}\")\n",
    "try:\n",
    "    etl_result = run_custom_etl_streaming(dataset_name=local_dataset_name, \n",
    "                                          target_column=target_column,\n",
    "                                         test_split = .01)\n",
    "    \n",
    "    X_train = etl_result.get(\"X_train\")\n",
    "    y_train = etl_result.get(\"y_train\")\n",
    "    X_val = etl_result.get(\"X_val\")\n",
    "    y_val = etl_result.get(\"y_val\")\n",
    "    X_test = etl_result.get(\"X_test\")\n",
    "    passenger_ids = etl_result.get(\"passenger_ids\")\n",
    "       \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the ETL process: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 2. Define Model and Parameters ---\n",
    "model_name = \"lightgbm\"\n",
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 10,\n",
    "    \"min_child_samples\": 10,\n",
    "    \"n_estimators\": 200,\n",
    "    \"num_leaves\": 31,\n",
    "    \"random_state\": 42 \n",
    "}\n",
    "\n",
    "# --- 3. Train the model ---\n",
    "print(f\"\\n--- Training {model_name} model ---\")\n",
    "model = get_sklearn_model(model_name, **params)\n",
    "\n",
    "# Ensure y_train and y_val are pandas Series for train_sklearn_model if it expects that\n",
    "if not isinstance(y_train, pd.Series):\n",
    "    y_train = pd.Series(y_train)\n",
    "if not isinstance(y_val, pd.Series):\n",
    "    y_val = pd.Series(y_val)\n",
    "    \n",
    "model, y_pred_val = train_sklearn_model(\n",
    "    model, X_train, y_train, X_val, y_val,\n",
    "    task_type=task_type,\n",
    ")\n",
    "print(f\"âœ… Finished training {model_name} model.\")\n",
    "\n",
    "# --- 5. Make Predictions on Test Data ---\n",
    "if X_test is not None and passenger_ids is not None:\n",
    "    print(\"\\n--- Making predictions on test data ---\")\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    target_is_boolean_flag = (task_type == \"binary_classification\") # Example logic for boolean target\n",
    "    \n",
    "    create_submission_file(\n",
    "        passenger_ids=passenger_ids,\n",
    "        predictions=y_pred_test,\n",
    "        target_column_name=target_column,\n",
    "        output_filename=\"submission.csv\",\n",
    "        target_is_boolean=target_is_boolean_flag\n",
    "    )\n",
    "else:\n",
    "    if X_test is None:\n",
    "        print(\"\\nâš ï¸ X_test is not available. Cannot generate submission file.\")\n",
    "    if passenger_ids is None:\n",
    "        print(\"\\nâš ï¸ passenger_ids are not available. Cannot generate submission file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
